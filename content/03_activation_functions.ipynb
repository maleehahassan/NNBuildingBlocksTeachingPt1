{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Activation Functions\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maleehahassan/NNBuildingBlocksTeachingPt1/blob/main/content/03_activation_functions.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you will understand:\n",
    "- Why activation functions are crucial in neural networks\n",
    "- The problems with linear activation functions\n",
    "- Common activation functions and their properties\n",
    "- How to choose the right activation function\n",
    "- The impact of activation functions on learning\n",
    "\n",
    "## Why Do We Need Activation Functions?\n",
    "\n",
    "In the previous section, we used a simple **step function** in our perceptron. But what if we want:\n",
    "- **Smooth gradients** for better learning?\n",
    "- **Probabilistic outputs** instead of hard 0/1?\n",
    "- **Non-linear decision boundaries**?\n",
    "\n",
    "This is where **activation functions** come to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit  # sigmoid function\n",
    "\n",
    "# Let's start by showing the problem with linear functions\n",
    "def demonstrate_linear_problem():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Linear network (just matrix multiplications)\n",
    "    ax1.set_title('Linear Network (Without Activation Functions)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Show that multiple linear layers = single linear layer\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    # Layer 1: y1 = 2x\n",
    "    y1 = 2 * x\n",
    "    ax1.plot(x, y1, 'b-', linewidth=2, label='Layer 1: 2x')\n",
    "    \n",
    "    # Layer 2: y2 = 3 * y1 = 3 * 2x = 6x\n",
    "    y2 = 3 * y1\n",
    "    ax1.plot(x, y2, 'r-', linewidth=2, label='Layer 2: 3×(2x) = 6x')\n",
    "    \n",
    "    # Equivalent single layer\n",
    "    y_equivalent = 6 * x\n",
    "    ax1.plot(x, y_equivalent, 'g--', linewidth=3, label='Equivalent: 6x', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Input')\n",
    "    ax1.set_ylabel('Output')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.text(0, 10, 'Multiple linear layers\\n= Single linear layer!', \n",
    "             ha='center', fontsize=12, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    # Non-linear network (with activation functions)\n",
    "    ax2.set_title('Non-Linear Network (With Activation Functions)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Layer 1 with activation\n",
    "    y1_nonlinear = np.tanh(2 * x)  # tanh activation\n",
    "    ax2.plot(x, y1_nonlinear, 'b-', linewidth=2, label='Layer 1: tanh(2x)')\n",
    "    \n",
    "    # Layer 2 with activation  \n",
    "    y2_nonlinear = np.tanh(3 * y1_nonlinear)\n",
    "    ax2.plot(x, y2_nonlinear, 'r-', linewidth=2, label='Layer 2: tanh(3×Layer1)')\n",
    "    \n",
    "    ax2.set_xlabel('Input')\n",
    "    ax2.set_ylabel('Output')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.text(0, 0.5, 'Each layer adds\\ncomplexity!', \n",
    "             ha='center', fontsize=12, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_linear_problem()\n",
    "\n",
    "print(\"Key Insight: Without activation functions, deep networks are just expensive linear models!\")\n",
    "print(\"Activation functions introduce NON-LINEARITY, enabling complex pattern recognition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Activation Functions\n",
    "\n",
    "Let's explore the most important activation functions used in neural networks:\n",
    "\n",
    "### 1. Step Function (Perceptron)\n",
    "$$f(x) = \\begin{cases} 1 & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}$$\n",
    "\n",
    "### 2. Sigmoid (Logistic)\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "### 3. Hyperbolic Tangent (tanh)\n",
    "$$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "### 4. Rectified Linear Unit (ReLU)\n",
    "$$f(x) = \\max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def step_function(x):\n",
    "    return (x >= 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Clip to prevent overflow\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Plot all activation functions\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "functions = [\n",
    "    (step_function, 'Step Function', 'Perceptron classic'),\n",
    "    (sigmoid, 'Sigmoid', 'Smooth, outputs (0,1)'),\n",
    "    (tanh, 'Hyperbolic Tangent', 'Smooth, outputs (-1,1)'),\n",
    "    (relu, 'ReLU', 'Modern favorite'),\n",
    "    (leaky_relu, 'Leaky ReLU', 'ReLU variant'),\n",
    "    (lambda x: np.where(x > 0, x, 0.1 * (np.exp(x) - 1)), 'ELU', 'Exponential Linear Unit')\n",
    "]\n",
    "\n",
    "for i, (func, name, description) in enumerate(functions):\n",
    "    y = func(x)\n",
    "    axes[i].plot(x, y, linewidth=3, color=f'C{i}')\n",
    "    axes[i].set_title(f'{name}\\n{description}', fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_xlabel('Input (x)')\n",
    "    axes[i].set_ylabel('Output f(x)')\n",
    "    axes[i].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[i].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each activation function has different properties and use cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Key Activation Functions\n",
    "\n",
    "### Sigmoid Function: The Smooth Perceptron\n",
    "\n",
    "The sigmoid function was revolutionary because it:\n",
    "- **Smooth and differentiable** (enables gradient-based learning)\n",
    "- **Outputs probabilities** (values between 0 and 1)\n",
    "- **S-shaped curve** (smooth transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed sigmoid analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "sig_x = sigmoid(x)\n",
    "\n",
    "# 1. Sigmoid function\n",
    "axes[0,0].plot(x, sig_x, 'b-', linewidth=3, label='σ(x) = 1/(1+e^(-x))')\n",
    "axes[0,0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[0,0].set_title('Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Input (x)')\n",
    "axes[0,0].set_ylabel('Output σ(x)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Add annotations\n",
    "axes[0,0].annotate('Saturated\\n(gradient ≈ 0)', xy=(-4, sigmoid(-4)), xytext=(-5, 0.3),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "axes[0,0].annotate('Saturated\\n(gradient ≈ 0)', xy=(4, sigmoid(4)), xytext=(3, 0.7),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "axes[0,0].annotate('Steep gradient\\n(fast learning)', xy=(0, 0.5), xytext=(1, 0.2),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "\n",
    "# 2. Sigmoid derivative (gradient)\n",
    "sigmoid_derivative = sig_x * (1 - sig_x)\n",
    "axes[0,1].plot(x, sigmoid_derivative, 'g-', linewidth=3, label=\"σ'(x) = σ(x)(1-σ(x))\")\n",
    "axes[0,1].set_title('Sigmoid Derivative (Gradient)', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Input (x)')\n",
    "axes[0,1].set_ylabel('Gradient')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. Comparison with step function\n",
    "step_x = step_function(x)\n",
    "axes[1,0].plot(x, step_x, 'r-', linewidth=3, label='Step function')\n",
    "axes[1,0].plot(x, sig_x, 'b-', linewidth=3, label='Sigmoid', alpha=0.7)\n",
    "axes[1,0].set_title('Step vs Sigmoid', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Input (x)')\n",
    "axes[1,0].set_ylabel('Output')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Sigmoid with different slopes\n",
    "for slope in [0.5, 1, 2, 5]:\n",
    "    y = sigmoid(slope * x)\n",
    "    axes[1,1].plot(x, y, linewidth=2, label=f'σ({slope}x)')\n",
    "axes[1,1].set_title('Sigmoid with Different Slopes', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Input (x)')\n",
    "axes[1,1].set_ylabel('Output')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid Properties:\")\n",
    "print(\"✓ Smooth and differentiable\")\n",
    "print(\"✓ Outputs between 0 and 1 (probabilities)\")\n",
    "print(\"✓ S-shaped curve\")\n",
    "print(\"✗ Vanishing gradient problem (saturates at extremes)\")\n",
    "print(\"✗ Not zero-centered (can slow learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU: The Modern Champion\n",
    "\n",
    "ReLU (Rectified Linear Unit) became the default choice because it:\n",
    "- **Simple**: f(x) = max(0, x)\n",
    "- **Fast**: Computationally efficient\n",
    "- **Avoids vanishing gradients**: Gradient is either 0 or 1\n",
    "- **Sparse**: Many neurons output 0 (computational efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed ReLU analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "relu_x = relu(x)\n",
    "\n",
    "# 1. ReLU function\n",
    "axes[0,0].plot(x, relu_x, 'r-', linewidth=3, label='ReLU(x) = max(0, x)')\n",
    "axes[0,0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0,0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0,0].set_title('ReLU Function', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Input (x)')\n",
    "axes[0,0].set_ylabel('Output')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Add annotations\n",
    "axes[0,0].annotate('Dead zone\\n(gradient = 0)', xy=(-1.5, 0), xytext=(-2, 1),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "axes[0,0].annotate('Active zone\\n(gradient = 1)', xy=(1.5, 1.5), xytext=(0.5, 2.5),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "\n",
    "# 2. ReLU derivative\n",
    "relu_derivative = (x > 0).astype(float)\n",
    "axes[0,1].plot(x, relu_derivative, 'g-', linewidth=3, label=\"ReLU'(x)\")\n",
    "axes[0,1].set_title('ReLU Derivative', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Input (x)')\n",
    "axes[0,1].set_ylabel('Gradient')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# 3. Comparison: Sigmoid vs ReLU\n",
    "axes[1,0].plot(x, sigmoid(x), 'b-', linewidth=3, label='Sigmoid', alpha=0.7)\n",
    "axes[1,0].plot(x, relu_x, 'r-', linewidth=3, label='ReLU')\n",
    "axes[1,0].set_title('Sigmoid vs ReLU', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Input (x)')\n",
    "axes[1,0].set_ylabel('Output')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. ReLU variants\n",
    "leaky_x = leaky_relu(x, 0.1)\n",
    "elu_x = np.where(x > 0, x, 0.1 * (np.exp(x) - 1))\n",
    "\n",
    "axes[1,1].plot(x, relu_x, 'r-', linewidth=3, label='ReLU')\n",
    "axes[1,1].plot(x, leaky_x, 'g-', linewidth=3, label='Leaky ReLU')\n",
    "axes[1,1].plot(x, elu_x, 'b-', linewidth=3, label='ELU')\n",
    "axes[1,1].set_title('ReLU Variants', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Input (x)')\n",
    "axes[1,1].set_ylabel('Output')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ReLU Properties:\")\n",
    "print(\"✓ Simple and fast computation\")\n",
    "print(\"✓ No vanishing gradient (for x > 0)\")\n",
    "print(\"✓ Sparse activation (many zeros)\")\n",
    "print(\"✓ Biologically inspired\")\n",
    "print(\"✗ Dying ReLU problem (neurons can become permanently inactive)\")\n",
    "print(\"✗ Not differentiable at x = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Demonstration: Impact on Learning\n",
    "\n",
    "Let's see how different activation functions affect the learning process in a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple neural network to compare activation functions\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, activation_func, activation_derivative, learning_rate=0.1):\n",
    "        self.activation = activation_func\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.w1 = np.random.randn(2, 3) * 0.5  # Input to hidden\n",
    "        self.b1 = np.random.randn(1, 3) * 0.5\n",
    "        self.w2 = np.random.randn(3, 1) * 0.5  # Hidden to output\n",
    "        self.b2 = np.random.randn(1, 1) * 0.5\n",
    "        \n",
    "        self.losses = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)  # Output layer always sigmoid for binary classification\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = output - y\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.w2.T)\n",
    "        dz1 = da1 * self.activation_derivative(self.z1)\n",
    "        dw1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.w2 -= self.learning_rate * dw2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.w1 -= self.learning_rate * dw1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = -np.mean(y * np.log(output + 1e-8) + (1 - y) * np.log(1 - output + 1e-8))\n",
    "            self.losses.append(loss)\n",
    "            self.backward(X, y, output)\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Generate XOR-like dataset (non-linearly separable)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create a more complex pattern\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = ((X[:, 0] * X[:, 1]) > 0).astype(float).reshape(-1, 1)\n",
    "\n",
    "# Train networks with different activation functions\n",
    "activations = [\n",
    "    (sigmoid, sigmoid_derivative, 'Sigmoid'),\n",
    "    (tanh, tanh_derivative, 'Tanh'),\n",
    "    (relu, relu_derivative, 'ReLU')\n",
    "]\n",
    "\n",
    "networks = []\n",
    "for activation, derivative, name in activations:\n",
    "    print(f\"Training network with {name} activation...\")\n",
    "    network = SimpleNeuralNetwork(activation, derivative, learning_rate=0.01)\n",
    "    network.train(X, y, epochs=500)\n",
    "    networks.append((network, name))\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Learning curves\n",
    "plt.subplot(2, 2, 1)\n",
    "for network, name in networks:\n",
    "    plt.plot(network.losses, linewidth=2, label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curves: Different Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Decision boundaries\n",
    "for i, (network, name) in enumerate(networks):\n",
    "    plt.subplot(2, 2, i + 2)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = network.forward(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='RdYlBu', edgecolors='black', alpha=0.7)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'Decision Boundary: {name}')\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate final accuracies\n",
    "print(\"\\nFinal Results:\")\n",
    "for network, name in networks:\n",
    "    predictions = (network.forward(X) > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    final_loss = network.losses[-1]\n",
    "    print(f\"{name:8s}: Accuracy = {accuracy:.3f}, Final Loss = {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Activation Function\n",
    "\n",
    "### Guidelines for Selection:\n",
    "\n",
    "#### Hidden Layers:\n",
    "- **ReLU**: Default choice for most cases\n",
    "  - Fast, simple, avoids vanishing gradients\n",
    "  - Good for deep networks\n",
    "  \n",
    "- **Leaky ReLU/ELU**: When ReLU causes \"dying\" neurons\n",
    "  - Addresses the dying ReLU problem\n",
    "  - Slightly more complex but often better performance\n",
    "  \n",
    "- **Tanh**: When you need zero-centered outputs\n",
    "  - Better than sigmoid for hidden layers\n",
    "  - Still suffers from vanishing gradients\n",
    "\n",
    "#### Output Layers:\n",
    "- **Sigmoid**: Binary classification (0/1 probabilities)\n",
    "- **Softmax**: Multi-class classification (probability distribution)\n",
    "- **Linear**: Regression (no activation)\n",
    "- **Tanh**: When outputs should be between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Activation': ['Step', 'Sigmoid', 'Tanh', 'ReLU', 'Leaky ReLU', 'ELU'],\n",
    "    'Range': ['[0, 1]', '(0, 1)', '(-1, 1)', '[0, ∞)', '(-∞, ∞)', '(-α, ∞)'],\n",
    "    'Differentiable': ['No', 'Yes', 'Yes', 'Almost', 'Almost', 'Yes'],\n",
    "    'Vanishing Gradient': ['N/A', 'Yes', 'Yes', 'No', 'No', 'No'],\n",
    "    'Computational Cost': ['Very Low', 'Medium', 'Medium', 'Very Low', 'Low', 'Medium'],\n",
    "    'Common Use': ['Historical', 'Output Layer', 'Hidden Layer', 'Hidden Layer', 'Hidden Layer', 'Hidden Layer'],\n",
    "    'Zero-Centered': ['No', 'No', 'Yes', 'No', 'No', 'Almost']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Activation Function Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Visual comparison of key properties\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "x = np.linspace(-2, 2, 1000)\n",
    "\n",
    "# Plot 1: Saturation comparison\n",
    "axes[0].plot(x, sigmoid(x), label='Sigmoid (saturates)', linewidth=3)\n",
    "axes[0].plot(x, tanh(x), label='Tanh (saturates)', linewidth=3)\n",
    "axes[0].plot(x, relu(x), label='ReLU (no saturation)', linewidth=3)\n",
    "axes[0].set_title('Saturation Behavior', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Input')\n",
    "axes[0].set_ylabel('Output')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Zero-centered comparison\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5, label='Zero line')\n",
    "axes[1].plot(x, sigmoid(x) - 0.5, label='Sigmoid (shifted)', linewidth=3)\n",
    "axes[1].plot(x, tanh(x), label='Tanh (zero-centered)', linewidth=3)\n",
    "axes[1].plot(x, relu(x) - 1, label='ReLU (shifted)', linewidth=3)\n",
    "axes[1].set_title('Zero-Centered Outputs', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Input')\n",
    "axes[1].set_ylabel('Output')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Gradient comparison\n",
    "sig_grad = sigmoid(x) * (1 - sigmoid(x))\n",
    "tanh_grad = 1 - tanh(x)**2\n",
    "relu_grad = (x > 0).astype(float)\n",
    "\n",
    "axes[2].plot(x, sig_grad, label='Sigmoid derivative', linewidth=3)\n",
    "axes[2].plot(x, tanh_grad, label='Tanh derivative', linewidth=3)\n",
    "axes[2].plot(x, relu_grad, label='ReLU derivative', linewidth=3)\n",
    "axes[2].set_title('Gradient Behavior', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Input')\n",
    "axes[2].set_ylabel('Gradient')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Problems and Solutions\n",
    "\n",
    "### 1. Vanishing Gradient Problem\n",
    "**Problem**: Gradients become very small in deep networks, making learning slow/impossible.\n",
    "\n",
    "**Causes**: \n",
    "- Sigmoid/tanh saturation\n",
    "- Many layers multiplying small gradients\n",
    "\n",
    "**Solutions**:\n",
    "- Use ReLU or variants\n",
    "- Proper weight initialization\n",
    "- Batch normalization\n",
    "- Residual connections\n",
    "\n",
    "### 2. Dying ReLU Problem\n",
    "**Problem**: ReLU neurons become permanently inactive (always output 0).\n",
    "\n",
    "**Causes**:\n",
    "- Large negative bias\n",
    "- Poor weight initialization\n",
    "- High learning rates\n",
    "\n",
    "**Solutions**:\n",
    "- Leaky ReLU (small negative slope)\n",
    "- ELU (exponential for negative values)\n",
    "- Proper initialization\n",
    "- Lower learning rates\n",
    "\n",
    "### 3. Exploding Gradient Problem\n",
    "**Problem**: Gradients become very large, causing unstable training.\n",
    "\n",
    "**Solutions**:\n",
    "- Gradient clipping\n",
    "- Proper weight initialization\n",
    "- Lower learning rates\n",
    "- Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradient problem\n",
    "def demonstrate_vanishing_gradients():\n",
    "    # Simulate gradient flow through multiple sigmoid layers\n",
    "    layers = [5, 10, 15, 20]\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, n_layers in enumerate(layers):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        \n",
    "        # Calculate gradient through n layers\n",
    "        gradient = np.ones_like(x)\n",
    "        activation = x.copy()\n",
    "        \n",
    "        # Forward pass through layers\n",
    "        activations = [activation]\n",
    "        for layer in range(n_layers):\n",
    "            activation = sigmoid(activation)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # Backward pass (chain rule)\n",
    "        for layer in range(n_layers):\n",
    "            # Sigmoid derivative: σ(x) * (1 - σ(x))\n",
    "            layer_gradient = activations[n_layers - layer] * (1 - activations[n_layers - layer])\n",
    "            gradient *= layer_gradient\n",
    "        \n",
    "        plt.plot(x, gradient, linewidth=3)\n",
    "        plt.title(f'Gradient after {n_layers} Sigmoid Layers')\n",
    "        plt.xlabel('Input')\n",
    "        plt.ylabel('Gradient Magnitude')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        # Show vanishing effect\n",
    "        max_gradient = np.max(gradient)\n",
    "        plt.text(0, max_gradient/2, f'Max gradient: {max_gradient:.2e}', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"As we add more sigmoid layers, gradients become exponentially smaller!\")\n",
    "    print(\"This makes it very difficult to train deep networks with sigmoid activation.\")\n",
    "\n",
    "demonstrate_vanishing_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Why Activation Functions Matter:\n",
    "1. **Enable non-linearity**: Without them, deep networks = linear models\n",
    "2. **Control information flow**: Determine what gets passed to next layer\n",
    "3. **Affect learning speed**: Gradient properties impact training\n",
    "4. **Shape decision boundaries**: Different functions → different capabilities\n",
    "\n",
    "### Modern Best Practices:\n",
    "1. **Start with ReLU** for hidden layers\n",
    "2. **Consider Leaky ReLU/ELU** if ReLU causes problems\n",
    "3. **Use sigmoid/softmax** for output layers (classification)\n",
    "4. **Avoid sigmoid/tanh** in deep networks (vanishing gradients)\n",
    "5. **Match activation to problem type**\n",
    "\n",
    "### Historical Evolution:\n",
    "- **1940s-1980s**: Step functions, early sigmoid\n",
    "- **1980s-2000s**: Sigmoid, tanh dominance\n",
    "- **2010s-present**: ReLU revolution\n",
    "- **Future**: Swish, GELU, learned activations\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. Why did ReLU become so popular despite being \"just\" max(0,x)?\n",
    "2. When might you still choose sigmoid over ReLU?\n",
    "3. How do activation functions relate to biological neurons?\n",
    "4. What problems might arise with very deep ReLU networks?\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: We'll explore **Loss Functions** - how neural networks measure and minimize their mistakes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}