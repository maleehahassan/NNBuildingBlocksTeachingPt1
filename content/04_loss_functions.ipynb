{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loss Functions\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maleehahassan/NNBuildingBlocksTeachingPt1/blob/main/content/04_loss_functions.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you will understand:\n",
    "- What loss functions are and why they're crucial\n",
    "- Common loss functions for different problem types\n",
    "- How loss functions guide the learning process\n",
    "- The relationship between loss functions and optimization\n",
    "- How to choose the right loss function for your problem\n",
    "\n",
    "## What is a Loss Function?\n",
    "\n",
    "A **loss function** (also called cost function or objective function) measures how \"wrong\" our model's predictions are. It:\n",
    "\n",
    "- **Quantifies the error** between predicted and actual values\n",
    "- **Guides the learning process** by telling us which direction to adjust weights\n",
    "- **Enables optimization** through gradient descent\n",
    "- **Defines what \"good\" means** for our specific problem\n",
    "\n",
    "### The Learning Process:\n",
    "1. Make predictions\n",
    "2. Calculate loss (how wrong we are)\n",
    "3. Compute gradients (which direction to improve)\n",
    "4. Update weights (take a step toward better predictions)\n",
    "5. Repeat until loss is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "# Visualize the concept of loss\n",
    "def visualize_loss_concept():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Example data\n",
    "    np.random.seed(42)\n",
    "    x = np.linspace(0, 10, 20)\n",
    "    y_true = 2 * x + 1 + np.random.normal(0, 2, len(x))  # True relationship with noise\n",
    "    \n",
    "    # Three different model predictions\n",
    "    models = [\n",
    "        (\"Bad Model\", 0.5 * x + 5),\n",
    "        (\"OK Model\", 1.5 * x + 2),\n",
    "        (\"Good Model\", 2.1 * x + 0.8)\n",
    "    ]\n",
    "    \n",
    "    colors = ['red', 'orange', 'green']\n",
    "    \n",
    "    for i, (name, y_pred) in enumerate(models):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot data and predictions\n",
    "        ax.scatter(x, y_true, color='blue', alpha=0.7, s=50, label='True Data')\n",
    "        ax.plot(x, y_pred, color=colors[i], linewidth=3, label=f'{name} Prediction')\n",
    "        \n",
    "        # Draw error lines\n",
    "        for xi, yi_true, yi_pred in zip(x, y_true, y_pred):\n",
    "            ax.plot([xi, xi], [yi_true, yi_pred], 'k--', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # Calculate and display loss (Mean Squared Error)\n",
    "        mse = np.mean((y_true - y_pred)**2)\n",
    "        ax.set_title(f'{name}\\nMSE Loss = {mse:.2f}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Input (x)')\n",
    "        ax.set_ylabel('Output (y)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Insight: Lower loss = better model performance!\")\n",
    "    print(\"The loss function gives us a single number to optimize.\")\n",
    "\n",
    "visualize_loss_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Regression\n",
    "\n",
    "Regression problems predict continuous values (prices, temperatures, etc.). Common loss functions:\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- **Most popular** for regression\n",
    "- **Penalizes large errors heavily** (squaring)\n",
    "- **Differentiable everywhere**\n",
    "- **Sensitive to outliers**\n",
    "\n",
    "### 2. Mean Absolute Error (MAE)\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "- **Less sensitive to outliers**\n",
    "- **Equal penalty for all errors**\n",
    "- **Not differentiable at zero**\n",
    "\n",
    "### 3. Huber Loss\n",
    "$$\\text{Huber}(\\delta) = \\begin{cases} \n",
    "\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "\\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "- **Combines MSE and MAE**\n",
    "- **Robust to outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and compare regression loss functions\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = np.abs(y_true - y_pred)\n",
    "    return np.mean(np.where(error <= delta, \n",
    "                           0.5 * error**2, \n",
    "                           delta * error - 0.5 * delta**2))\n",
    "\n",
    "# Visualize how different loss functions respond to errors\n",
    "errors = np.linspace(-5, 5, 1000)\n",
    "mse_values = errors**2\n",
    "mae_values = np.abs(errors)\n",
    "huber_values = np.where(np.abs(errors) <= 1, \n",
    "                       0.5 * errors**2, \n",
    "                       np.abs(errors) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Loss function shapes\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(errors, mse_values, 'r-', linewidth=3, label='MSE (Squared Error)')\n",
    "plt.plot(errors, mae_values, 'b-', linewidth=3, label='MAE (Absolute Error)')\n",
    "plt.plot(errors, huber_values, 'g-', linewidth=3, label='Huber Loss (Î´=1)')\n",
    "plt.xlabel('Error (y_true - y_pred)')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Regression Loss Functions', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "# Plot 2: Sensitivity to outliers demonstration\n",
    "plt.subplot(2, 2, 2)\n",
    "# Normal data\n",
    "y_true_normal = np.array([1, 2, 3, 4, 5])\n",
    "y_pred_normal = np.array([1.1, 2.2, 2.8, 4.1, 4.9])\n",
    "\n",
    "# Data with outlier\n",
    "y_true_outlier = np.array([1, 2, 3, 4, 15])  # Last point is outlier\n",
    "y_pred_outlier = np.array([1.1, 2.2, 2.8, 4.1, 4.9])\n",
    "\n",
    "scenarios = ['Normal Data', 'With Outlier']\n",
    "y_true_scenarios = [y_true_normal, y_true_outlier]\n",
    "y_pred_scenarios = [y_pred_normal, y_pred_outlier]\n",
    "\n",
    "mse_values = []\n",
    "mae_values = []\n",
    "huber_values = []\n",
    "\n",
    "for y_true, y_pred in zip(y_true_scenarios, y_pred_scenarios):\n",
    "    mse_values.append(mse_loss(y_true, y_pred))\n",
    "    mae_values.append(mae_loss(y_true, y_pred))\n",
    "    huber_values.append(huber_loss(y_true, y_pred))\n",
    "\n",
    "x_pos = np.arange(len(scenarios))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x_pos - width, mse_values, width, label='MSE', color='red', alpha=0.7)\n",
    "plt.bar(x_pos, mae_values, width, label='MAE', color='blue', alpha=0.7)\n",
    "plt.bar(x_pos + width, huber_values, width, label='Huber', color='green', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Outlier Sensitivity Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x_pos, scenarios)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Gradient comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "# MSE gradient: 2 * error\n",
    "mse_grad = 2 * errors\n",
    "# MAE gradient: sign(error)\n",
    "mae_grad = np.sign(errors)\n",
    "# Huber gradient\n",
    "huber_grad = np.where(np.abs(errors) <= 1, errors, np.sign(errors))\n",
    "\n",
    "plt.plot(errors, mse_grad, 'r-', linewidth=3, label='MSE Gradient')\n",
    "plt.plot(errors, mae_grad, 'b-', linewidth=3, label='MAE Gradient')\n",
    "plt.plot(errors, huber_grad, 'g-', linewidth=3, label='Huber Gradient')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Gradient')\n",
    "plt.title('Loss Function Gradients', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "# Plot 4: Real example with outliers\n",
    "plt.subplot(2, 2, 4)\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 50)\n",
    "y_clean = 2 * x + 1 + np.random.normal(0, 1, len(x))\n",
    "y_outliers = y_clean.copy()\n",
    "y_outliers[10] += 15  # Add outlier\n",
    "y_outliers[30] -= 12  # Add another outlier\n",
    "\n",
    "plt.scatter(x, y_outliers, alpha=0.7)\n",
    "plt.scatter(x[10], y_outliers[10], color='red', s=100, label='Outliers')\n",
    "plt.scatter(x[30], y_outliers[30], color='red', s=100)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Dataset with Outliers', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss Function Characteristics:\")\n",
    "print(\"MSE: Heavily penalizes large errors, sensitive to outliers\")\n",
    "print(\"MAE: Treats all errors equally, robust to outliers\")\n",
    "print(\"Huber: Compromise between MSE and MAE\")\n",
    "print(f\"\\nWith outliers - MSE: {mse_values[1]:.2f}, MAE: {mae_values[1]:.2f}, Huber: {huber_values[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Classification\n",
    "\n",
    "Classification problems predict discrete categories. Different loss functions for different scenarios:\n",
    "\n",
    "### 1. Binary Cross-Entropy (Log Loss)\n",
    "For binary classification (two classes):\n",
    "$$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "- **Outputs probabilities** (0 to 1)\n",
    "- **Heavily penalizes confident wrong predictions**\n",
    "- **Smooth and differentiable**\n",
    "\n",
    "### 2. Categorical Cross-Entropy\n",
    "For multi-class classification:\n",
    "$$\\text{CCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$$\n",
    "\n",
    "### 3. Sparse Categorical Cross-Entropy\n",
    "Same as categorical, but with integer labels instead of one-hot encoding.\n",
    "\n",
    "### 4. Hinge Loss\n",
    "Used in Support Vector Machines:\n",
    "$$\\text{Hinge} = \\max(0, 1 - y \\cdot \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and visualize classification loss functions\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Clip predictions to prevent log(0)\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    # Convert 0/1 labels to -1/1 for hinge loss\n",
    "    y_true_hinge = 2 * y_true - 1\n",
    "    y_pred_hinge = 2 * y_pred - 1\n",
    "    return np.mean(np.maximum(0, 1 - y_true_hinge * y_pred_hinge))\n",
    "\n",
    "# Visualize binary classification loss functions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Generate probability predictions\n",
    "probabilities = np.linspace(0.001, 0.999, 1000)\n",
    "\n",
    "# Plot 1: Binary Cross-Entropy for different true labels\n",
    "ax1 = axes[0, 0]\n",
    "bce_positive = -np.log(probabilities)  # y_true = 1\n",
    "bce_negative = -np.log(1 - probabilities)  # y_true = 0\n",
    "\n",
    "ax1.plot(probabilities, bce_positive, 'b-', linewidth=3, label='True class = 1')\n",
    "ax1.plot(probabilities, bce_negative, 'r-', linewidth=3, label='True class = 0')\n",
    "ax1.set_xlabel('Predicted Probability')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Binary Cross-Entropy Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Add annotations\n",
    "ax1.annotate('Confident wrong\\nprediction = high loss', \n",
    "            xy=(0.1, -np.log(0.1)), xytext=(0.3, 5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "ax1.annotate('Confident correct\\nprediction = low loss', \n",
    "            xy=(0.9, -np.log(0.9)), xytext=(0.7, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "\n",
    "# Plot 2: Comparison of classification losses\n",
    "ax2 = axes[0, 1]\n",
    "# For true class = 1\n",
    "y_true_ones = np.ones_like(probabilities)\n",
    "bce_loss = binary_cross_entropy(y_true_ones, probabilities)\n",
    "hinge_loss_values = np.maximum(0, 1 - (2 * probabilities - 1))\n",
    "zero_one_loss = (probabilities < 0.5).astype(float)  # 0-1 loss\n",
    "\n",
    "ax2.plot(probabilities, bce_positive, 'b-', linewidth=3, label='Cross-Entropy')\n",
    "ax2.plot(probabilities, hinge_loss_values, 'g-', linewidth=3, label='Hinge Loss')\n",
    "ax2.plot(probabilities, zero_one_loss, 'r-', linewidth=3, label='0-1 Loss')\n",
    "ax2.set_xlabel('Predicted Probability (True class = 1)')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Classification Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# Plot 3: Real classification example\n",
    "ax3 = axes[1, 0]\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y_true = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Simulate predictions with different confidence levels\n",
    "predictions_good = y_true + np.random.normal(0, 0.1, n_samples)\n",
    "predictions_good = np.clip(predictions_good, 0.01, 0.99)\n",
    "\n",
    "predictions_bad = 0.5 + np.random.normal(0, 0.1, n_samples)  # Random predictions\n",
    "predictions_bad = np.clip(predictions_bad, 0.01, 0.99)\n",
    "\n",
    "bce_good = binary_cross_entropy(y_true, predictions_good)\n",
    "bce_bad = binary_cross_entropy(y_true, predictions_bad)\n",
    "\n",
    "models = ['Good Model', 'Bad Model']\n",
    "losses = [bce_good, bce_bad]\n",
    "colors = ['green', 'red']\n",
    "\n",
    "bars = ax3.bar(models, losses, color=colors, alpha=0.7)\n",
    "ax3.set_ylabel('Binary Cross-Entropy Loss')\n",
    "ax3.set_title('Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, loss in zip(bars, losses):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Multi-class visualization\n",
    "ax4 = axes[1, 1]\n",
    "# Demonstrate categorical cross-entropy for 3 classes\n",
    "classes = ['Class A', 'Class B', 'Class C']\n",
    "true_class = 0  # True class is A\n",
    "\n",
    "# Different prediction scenarios\n",
    "scenarios = [\n",
    "    ([0.8, 0.1, 0.1], 'Confident Correct'),\n",
    "    ([0.4, 0.3, 0.3], 'Uncertain Correct'),\n",
    "    ([0.1, 0.8, 0.1], 'Confident Wrong'),\n",
    "    ([0.33, 0.33, 0.34], 'Random Guess')\n",
    "]\n",
    "\n",
    "scenario_names = [name for _, name in scenarios]\n",
    "losses = []\n",
    "\n",
    "for pred, name in scenarios:\n",
    "    # Categorical cross-entropy for true class 0\n",
    "    loss = -np.log(pred[true_class])\n",
    "    losses.append(loss)\n",
    "\n",
    "bars = ax4.bar(scenario_names, losses, color=['green', 'yellow', 'red', 'orange'], alpha=0.7)\n",
    "ax4.set_ylabel('Categorical Cross-Entropy Loss')\n",
    "ax4.set_title('Multi-class Classification Loss', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.setp(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, loss in zip(bars, losses):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{loss:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Loss Insights:\")\n",
    "print(\"â¢ Cross-entropy heavily penalizes confident wrong predictions\")\n",
    "print(\"â¢ Provides smooth gradients for optimization\")\n",
    "print(\"â¢ Encourages probability outputs (useful for uncertainty)\")\n",
    "print(\"â¢ Hinge loss focuses on margin (decision boundary distance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Connection to Optimization\n",
    "\n",
    "Loss functions are not just measurement tools - they **drive the learning process** through optimization algorithms like gradient descent.\n",
    "\n",
    "### Gradient Descent Process:\n",
    "1. **Forward Pass**: Calculate predictions and loss\n",
    "2. **Backward Pass**: Compute gradients of loss w.r.t. weights\n",
    "3. **Weight Update**: Move weights in direction that reduces loss\n",
    "4. **Repeat**: Until convergence or stopping criteria\n",
    "\n",
    "### Mathematical Foundation:\n",
    "$$\\mathbf{w}_{new} = \\mathbf{w}_{old} - \\eta \\nabla_\\mathbf{w} L$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{w}$ = weights\n",
    "- $\\eta$ = learning rate\n",
    "- $\\nabla_\\mathbf{w} L$ = gradient of loss w.r.t. weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient descent with different loss functions\n",
    "def demonstrate_gradient_descent():\n",
    "    # Simple 1D optimization problem\n",
    "    # True function: y = 3x + 2\n",
    "    # We'll learn the slope (weight) starting from a random guess\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    x_data = np.random.randn(100)\n",
    "    y_data = 3 * x_data + 2 + np.random.normal(0, 0.5, 100)\n",
    "    \n",
    "    def mse_gradient(w, x, y):\n",
    "        \"\"\"Gradient of MSE loss w.r.t. weight w\"\"\"\n",
    "        y_pred = w * x\n",
    "        return 2 * np.mean((y_pred - y) * x)\n",
    "    \n",
    "    def mae_gradient(w, x, y):\n",
    "        \"\"\"Gradient of MAE loss w.r.t. weight w\"\"\"\n",
    "        y_pred = w * x\n",
    "        return np.mean(np.sign(y_pred - y) * x)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w_mse = 0.0\n",
    "    w_mae = 0.0\n",
    "    learning_rate = 0.01\n",
    "    epochs = 100\n",
    "    \n",
    "    # Track progress\n",
    "    weights_mse = [w_mse]\n",
    "    weights_mae = [w_mae]\n",
    "    losses_mse = []\n",
    "    losses_mae = []\n",
    "    \n",
    "    # Gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        # MSE optimization\n",
    "        grad_mse = mse_gradient(w_mse, x_data, y_data)\n",
    "        w_mse -= learning_rate * grad_mse\n",
    "        loss_mse = np.mean((w_mse * x_data - y_data)**2)\n",
    "        \n",
    "        # MAE optimization  \n",
    "        grad_mae = mae_gradient(w_mae, x_data, y_data)\n",
    "        w_mae -= learning_rate * grad_mae\n",
    "        loss_mae = np.mean(np.abs(w_mae * x_data - y_data))\n",
    "        \n",
    "        weights_mse.append(w_mse)\n",
    "        weights_mae.append(w_mae)\n",
    "        losses_mse.append(loss_mse)\n",
    "        losses_mae.append(loss_mae)\n",
    "    \n",
    "    # Visualize optimization process\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Weight convergence\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(weights_mse, 'b-', linewidth=2, label='MSE Optimization')\n",
    "    ax1.plot(weights_mae, 'r-', linewidth=2, label='MAE Optimization')\n",
    "    ax1.axhline(y=3, color='green', linestyle='--', linewidth=2, label='True Weight = 3')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Weight Value')\n",
    "    ax1.set_title('Weight Convergence', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss convergence\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(losses_mse, 'b-', linewidth=2, label='MSE Loss')\n",
    "    ax2.plot(losses_mae, 'r-', linewidth=2, label='MAE Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_title('Loss Convergence', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Plot 3: Loss landscape\n",
    "    ax3 = axes[1, 0]\n",
    "    weight_range = np.linspace(-1, 5, 100)\n",
    "    mse_landscape = [np.mean((w * x_data - y_data)**2) for w in weight_range]\n",
    "    mae_landscape = [np.mean(np.abs(w * x_data - y_data)) for w in weight_range]\n",
    "    \n",
    "    ax3.plot(weight_range, mse_landscape, 'b-', linewidth=3, label='MSE Loss')\n",
    "    ax3.plot(weight_range, mae_landscape, 'r-', linewidth=3, label='MAE Loss')\n",
    "    ax3.axvline(x=3, color='green', linestyle='--', linewidth=2, label='True Weight')\n",
    "    \n",
    "    # Show optimization paths\n",
    "    mse_path_losses = [np.mean((w * x_data - y_data)**2) for w in weights_mse[::10]]\n",
    "    mae_path_losses = [np.mean(np.abs(w * x_data - y_data)) for w in weights_mae[::10]]\n",
    "    \n",
    "    ax3.plot(weights_mse[::10], mse_path_losses, 'bo', markersize=8, alpha=0.7)\n",
    "    ax3.plot(weights_mae[::10], mae_path_losses, 'ro', markersize=8, alpha=0.7)\n",
    "    \n",
    "    ax3.set_xlabel('Weight Value')\n",
    "    ax3.set_ylabel('Loss Value')\n",
    "    ax3.set_title('Loss Landscape & Optimization Path', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Final predictions\n",
    "    ax4 = axes[1, 1]\n",
    "    x_test = np.linspace(-3, 3, 100)\n",
    "    y_true_line = 3 * x_test + 2\n",
    "    y_pred_mse = weights_mse[-1] * x_test\n",
    "    y_pred_mae = weights_mae[-1] * x_test\n",
    "    \n",
    "    ax4.scatter(x_data, y_data, alpha=0.5, label='Training Data')\n",
    "    ax4.plot(x_test, y_true_line, 'g-', linewidth=3, label='True Function')\n",
    "    ax4.plot(x_test, y_pred_mse, 'b--', linewidth=2, label=f'MSE Result (w={weights_mse[-1]:.2f})')\n",
    "    ax4.plot(x_test, y_pred_mae, 'r--', linewidth=2, label=f'MAE Result (w={weights_mae[-1]:.2f})')\n",
    "    \n",
    "    ax4.set_xlabel('x')\n",
    "    ax4.set_ylabel('y')\n",
    "    ax4.set_title('Final Learned Functions', fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final Results:\")\n",
    "    print(f\"True weight: 3.00\")\n",
    "    print(f\"MSE learned weight: {weights_mse[-1]:.3f}\")\n",
    "    print(f\"MAE learned weight: {weights_mae[-1]:.3f}\")\n",
    "    print(f\"MSE final loss: {losses_mse[-1]:.4f}\")\n",
    "    print(f\"MAE final loss: {losses_mae[-1]:.4f}\")\n",
    "\n",
    "demonstrate_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Loss Functions\n",
    "\n",
    "Modern deep learning uses specialized loss functions for specific problems:\n",
    "\n",
    "### 1. Focal Loss\n",
    "For imbalanced classification problems:\n",
    "$$\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "- **Focuses on hard examples**\n",
    "- **Reduces impact of easy examples**\n",
    "- **Addresses class imbalance**\n",
    "\n",
    "### 2. Dice Loss\n",
    "For image segmentation:\n",
    "$$\\text{Dice} = 1 - \\frac{2|A \\cap B|}{|A| + |B|}$$\n",
    "\n",
    "### 3. Contrastive Loss\n",
    "For similarity learning:\n",
    "$$L = \\frac{1}{2N} \\sum_{n=1}^{N} [y d^2 + (1-y) \\max(0, m-d)^2]$$\n",
    "\n",
    "### 4. Triplet Loss\n",
    "For embedding learning:\n",
    "$$L = \\max(0, d(a,p) - d(a,n) + \\text{margin})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate focal loss for imbalanced classification\n",
    "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"Focal Loss implementation\"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Calculate focal loss\n",
    "    ce_loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    focal_weight = (1 - p_t) ** gamma\n",
    "    \n",
    "    return np.mean(alpha * focal_weight * ce_loss)\n",
    "\n",
    "# Compare standard cross-entropy with focal loss\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Focal loss vs cross-entropy for different confidence levels\n",
    "ax1 = axes[0, 0]\n",
    "probs = np.linspace(0.01, 0.99, 100)\n",
    "y_true_positive = np.ones_like(probs)\n",
    "\n",
    "ce_loss = binary_cross_entropy(y_true_positive, probs)\n",
    "fl_loss_gamma1 = [focal_loss(np.array([1]), np.array([p]), gamma=1.0) for p in probs]\n",
    "fl_loss_gamma2 = [focal_loss(np.array([1]), np.array([p]), gamma=2.0) for p in probs]\n",
    "fl_loss_gamma5 = [focal_loss(np.array([1]), np.array([p]), gamma=5.0) for p in probs]\n",
    "\n",
    "ax1.plot(probs, -np.log(probs), 'k-', linewidth=3, label='Cross-Entropy')\n",
    "ax1.plot(probs, fl_loss_gamma1, 'b-', linewidth=2, label='Focal (Î³=1)')\n",
    "ax1.plot(probs, fl_loss_gamma2, 'r-', linewidth=2, label='Focal (Î³=2)')\n",
    "ax1.plot(probs, fl_loss_gamma5, 'g-', linewidth=2, label='Focal (Î³=5)')\n",
    "\n",
    "ax1.set_xlabel('Predicted Probability (True class = 1)')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Focal Loss vs Cross-Entropy', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Effect on easy vs hard examples\n",
    "ax2 = axes[0, 1]\n",
    "# Easy examples (high confidence correct predictions)\n",
    "easy_probs = np.array([0.9, 0.95, 0.99])\n",
    "# Hard examples (low confidence correct predictions)\n",
    "hard_probs = np.array([0.6, 0.7, 0.8])\n",
    "\n",
    "easy_ce = -np.log(easy_probs)\n",
    "hard_ce = -np.log(hard_probs)\n",
    "easy_focal = [(1-p)**2 * (-np.log(p)) for p in easy_probs]\n",
    "hard_focal = [(1-p)**2 * (-np.log(p)) for p in hard_probs]\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, easy_ce, width, label='Easy Examples (CE)', color='lightblue', alpha=0.7)\n",
    "ax2.bar(x + width/2, easy_focal, width, label='Easy Examples (Focal)', color='blue', alpha=0.7)\n",
    "ax2.bar(x + 3 - width/2, hard_ce, width, label='Hard Examples (CE)', color='lightcoral', alpha=0.7)\n",
    "ax2.bar(x + 3 + width/2, hard_focal, width, label='Hard Examples (Focal)', color='red', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Example Index')\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_title('Easy vs Hard Examples', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks([1, 4])\n",
    "ax2.set_xticklabels(['Easy Examples\\n(0.9, 0.95, 0.99)', 'Hard Examples\\n(0.6, 0.7, 0.8)'])\n",
    "\n",
    "# Plot 3: Imbalanced dataset simulation\n",
    "ax3 = axes[1, 0]\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create imbalanced dataset (10% positive class)\n",
    "n_samples = 1000\n",
    "n_positive = 100\n",
    "n_negative = 900\n",
    "\n",
    "# Simulate predictions (model struggles with minority class)\n",
    "positive_preds = np.random.beta(2, 3, n_positive)  # Skewed toward lower probabilities\n",
    "negative_preds = np.random.beta(1, 4, n_negative)  # Skewed toward lower probabilities\n",
    "\n",
    "y_true_imbalanced = np.concatenate([np.ones(n_positive), np.zeros(n_negative)])\n",
    "y_pred_imbalanced = np.concatenate([positive_preds, negative_preds])\n",
    "\n",
    "# Calculate losses\n",
    "ce_loss_imbalanced = binary_cross_entropy(y_true_imbalanced, y_pred_imbalanced)\n",
    "focal_loss_imbalanced = focal_loss(y_true_imbalanced, y_pred_imbalanced, gamma=2.0)\n",
    "\n",
    "losses = [ce_loss_imbalanced, focal_loss_imbalanced]\n",
    "loss_names = ['Cross-Entropy', 'Focal Loss']\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "bars = ax3.bar(loss_names, losses, color=colors, alpha=0.7)\n",
    "ax3.set_ylabel('Average Loss')\n",
    "ax3.set_title('Imbalanced Dataset (10% positive class)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, loss in zip(bars, losses):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{loss:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Class-wise loss breakdown\n",
    "ax4 = axes[1, 1]\n",
    "# Calculate per-class losses\n",
    "positive_mask = y_true_imbalanced == 1\n",
    "negative_mask = y_true_imbalanced == 0\n",
    "\n",
    "ce_pos = np.mean(-np.log(np.clip(y_pred_imbalanced[positive_mask], 1e-15, 1)))\n",
    "ce_neg = np.mean(-np.log(np.clip(1 - y_pred_imbalanced[negative_mask], 1e-15, 1)))\n",
    "\n",
    "focal_pos = np.mean([focal_loss(np.array([1]), np.array([p]), gamma=2.0) \n",
    "                    for p in y_pred_imbalanced[positive_mask]])\n",
    "focal_neg = np.mean([focal_loss(np.array([0]), np.array([p]), gamma=2.0) \n",
    "                    for p in y_pred_imbalanced[negative_mask]])\n",
    "\n",
    "classes = ['Positive Class\\n(minority)', 'Negative Class\\n(majority)']\n",
    "ce_losses = [ce_pos, ce_neg]\n",
    "focal_losses = [focal_pos, focal_neg]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "\n",
    "ax4.bar(x - width/2, ce_losses, width, label='Cross-Entropy', alpha=0.7)\n",
    "ax4.bar(x + width/2, focal_losses, width, label='Focal Loss', alpha=0.7)\n",
    "\n",
    "ax4.set_ylabel('Average Loss per Class')\n",
    "ax4.set_title('Per-Class Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(classes)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Focal Loss Benefits:\")\n",
    "print(\"â¢ Reduces loss for easy examples (well-classified)\")\n",
    "print(\"â¢ Maintains high loss for hard examples (misclassified)\")\n",
    "print(\"â¢ Helps with class imbalance by focusing on difficult cases\")\n",
    "print(f\"â¢ In this example: CE loss = {ce_loss_imbalanced:.4f}, Focal loss = {focal_loss_imbalanced:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Loss Function\n",
    "\n",
    "### Decision Framework:\n",
    "\n",
    "#### Problem Type:\n",
    "1. **Regression** â MSE, MAE, Huber\n",
    "2. **Binary Classification** â Binary Cross-Entropy, Hinge\n",
    "3. **Multi-class Classification** â Categorical Cross-Entropy\n",
    "4. **Multi-label Classification** â Binary Cross-Entropy per label\n",
    "\n",
    "#### Data Characteristics:\n",
    "- **Outliers present** â MAE, Huber Loss\n",
    "- **Class imbalance** â Focal Loss, Weighted Cross-Entropy\n",
    "- **Need probabilities** â Cross-Entropy based losses\n",
    "- **Need margins** â Hinge Loss, SVM-style losses\n",
    "\n",
    "#### Optimization Considerations:\n",
    "- **Smooth gradients needed** â Cross-Entropy, MSE\n",
    "- **Robust to noise** â Huber, MAE\n",
    "- **Fast convergence** â Well-conditioned losses\n",
    "\n",
    "### Common Pitfalls:\n",
    "1. **Wrong loss for problem type** (MSE for classification)\n",
    "2. **Ignoring class imbalance** (standard CE for imbalanced data)\n",
    "3. **Not considering outliers** (MSE with noisy data)\n",
    "4. **Inappropriate output activation** (sigmoid with MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison table\n",
    "import pandas as pd\n",
    "\n",
    "loss_comparison = {\n",
    "    'Loss Function': [\n",
    "        'Mean Squared Error (MSE)',\n",
    "        'Mean Absolute Error (MAE)', \n",
    "        'Huber Loss',\n",
    "        'Binary Cross-Entropy',\n",
    "        'Categorical Cross-Entropy',\n",
    "        'Hinge Loss',\n",
    "        'Focal Loss'\n",
    "    ],\n",
    "    'Problem Type': [\n",
    "        'Regression',\n",
    "        'Regression',\n",
    "        'Regression',\n",
    "        'Binary Classification',\n",
    "        'Multi-class Classification',\n",
    "        'Binary Classification',\n",
    "        'Imbalanced Classification'\n",
    "    ],\n",
    "    'Output Range': [\n",
    "        '[0, â)',\n",
    "        '[0, â)',\n",
    "        '[0, â)',\n",
    "        '[0, â)',\n",
    "        '[0, â)',\n",
    "        '[0, â)',\n",
    "        '[0, â)'\n",
    "    ],\n",
    "    'Outlier Sensitivity': [\n",
    "        'High',\n",
    "        'Low',\n",
    "        'Medium',\n",
    "        'Medium',\n",
    "        'Medium',\n",
    "        'Low',\n",
    "        'Adaptive'\n",
    "    ],\n",
    "    'Probabilistic Output': [\n",
    "        'No',\n",
    "        'No',\n",
    "        'No',\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        'No',\n",
    "        'Yes'\n",
    "    ],\n",
    "    'Common Use Cases': [\n",
    "        'General regression, least squares',\n",
    "        'Robust regression, outlier-prone data',\n",
    "        'Regression with some outliers',\n",
    "        'Binary classification, probability estimation',\n",
    "        'Multi-class classification',\n",
    "        'SVMs, margin-based classification',\n",
    "        'Imbalanced datasets, object detection'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(loss_comparison)\n",
    "print(\"Loss Function Comparison Guide:\")\n",
    "print(\"=\" * 120)\n",
    "print(df.to_string(index=False, max_colwidth=50))\n",
    "\n",
    "# Quick selection flowchart\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK SELECTION GUIDE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"ð REGRESSION PROBLEMS:\")\n",
    "print(\"   â¢ Normal data, no outliers â MSE\")\n",
    "print(\"   â¢ Data with outliers â MAE or Huber Loss\")\n",
    "print(\"   â¢ Need smooth gradients â MSE or Huber\")\n",
    "print()\n",
    "print(\"ð¯ CLASSIFICATION PROBLEMS:\")\n",
    "print(\"   â¢ Binary classification â Binary Cross-Entropy\")\n",
    "print(\"   â¢ Multi-class classification â Categorical Cross-Entropy\")\n",
    "print(\"   â¢ Imbalanced classes â Focal Loss or Weighted CE\")\n",
    "print(\"   â¢ Need decision margins â Hinge Loss\")\n",
    "print()\n",
    "print(\"â¡ SPECIAL CASES:\")\n",
    "print(\"   â¢ Object detection â Focal Loss\")\n",
    "print(\"   â¢ Image segmentation â Dice Loss\")\n",
    "print(\"   â¢ Similarity learning â Contrastive/Triplet Loss\")\n",
    "print(\"   â¢ Ranking problems â Pairwise/Listwise losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Loss Functions Are Essential Because They:\n",
    "1. **Define the learning objective** - what \"good\" means\n",
    "2. **Guide optimization** - provide gradients for weight updates\n",
    "3. **Match problem requirements** - regression vs classification\n",
    "4. **Handle data characteristics** - outliers, imbalance, noise\n",
    "5. **Enable different behaviors** - probability vs margin-based\n",
    "\n",
    "### Best Practices:\n",
    "1. **Match loss to problem type** (don't use MSE for classification)\n",
    "2. **Consider data characteristics** (outliers, imbalance)\n",
    "3. **Ensure compatible output activation** (sigmoid + BCE)\n",
    "4. **Monitor during training** (loss should generally decrease)\n",
    "5. **Use appropriate metrics** (loss â  always the best evaluation metric)\n",
    "\n",
    "### Common Mistakes to Avoid:\n",
    "- Using MSE for classification problems\n",
    "- Ignoring class imbalance in loss selection\n",
    "- Not preprocessing data appropriately for chosen loss\n",
    "- Confusing loss functions with evaluation metrics\n",
    "- Not considering computational efficiency for large datasets\n",
    "\n",
    "### The Big Picture:\n",
    "Loss functions are the **bridge between problem definition and optimization**. They translate our high-level goals (\"classify images correctly\", \"predict house prices accurately\") into mathematical objectives that computers can optimize.\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. Why can't we just use 0-1 loss (number of mistakes) for classification?\n",
    "2. When might you want to design a custom loss function?\n",
    "3. How do loss functions relate to the business objectives of a project?\n",
    "4. What happens if you use the \"wrong\" loss function?\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Put it all together with **Hands-on Exercises** where you'll implement and experiment with these concepts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}