{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hands-on Exercises\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maleehahassan/NNBuildingBlocksTeachingPt1/blob/main/content/05_hands_on_exercises.ipynb)\n",
    "\n",
    "## Workshop Summary & Practice\n",
    "\n",
    "Congratulations! You've learned about the fundamental building blocks of neural networks:\n",
    "\n",
    "1. **Supervised Learning** - Learning from labeled examples\n",
    "2. **Perceptron** - The first artificial neuron and its limitations\n",
    "3. **Activation Functions** - Adding non-linearity to enable complex patterns\n",
    "4. **Loss Functions** - Measuring and minimizing prediction errors\n",
    "\n",
    "Now it's time to **put it all together** with hands-on exercises!\n",
    "\n",
    "## Exercise Overview\n",
    "\n",
    "In the next 10 minutes, you'll:\n",
    "- Build a simple neural network from scratch\n",
    "- Experiment with different activation functions\n",
    "- Compare different loss functions\n",
    "- See how all the concepts work together\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Apply the concepts you've learned\n",
    "- Build intuition through experimentation\n",
    "- Understand how components interact\n",
    "- Gain confidence in neural network basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Setup - Import required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification, make_regression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Setup - Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üöÄ Welcome to the Neural Networks Building Blocks Workshop!\")\n",
    "print(\"Let's put everything we've learned into practice!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build Your First Neural Network (3 minutes)\n",
    "\n",
    "Let's build a simple neural network class that incorporates all the concepts we've learned.\n",
    "\n",
    "### üéØ Your Task:\n",
    "Complete the missing parts in the neural network implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu', loss='mse'):\n",
    "        \"\"\"\n",
    "        Initialize a simple 2-layer neural network\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: number of input features\n",
    "        - hidden_size: number of neurons in hidden layer\n",
    "        - output_size: number of output neurons\n",
    "        - activation: activation function ('relu', 'sigmoid', 'tanh')\n",
    "        - loss: loss function ('mse', 'binary_crossentropy')\n",
    "        \"\"\"\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.loss_function = loss\n",
    "        self.losses = []\n",
    "    \n",
    "    def activate(self, z, activation=None):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if activation is None:\n",
    "            activation = self.activation\n",
    "            \n",
    "        if activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "    def activate_derivative(self, z, activation=None):\n",
    "        \"\"\"Compute derivative of activation function\"\"\"\n",
    "        if activation is None:\n",
    "            activation = self.activation\n",
    "            \n",
    "        if activation == 'relu':\n",
    "            return (z > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            s = self.activate(z, 'sigmoid')\n",
    "            return s * (1 - s)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - np.tanh(z)**2\n",
    "        else:\n",
    "            return np.ones_like(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: z1 = X @ W1 + b1, a1 = activate(z1), z2 = a1 @ W2 + b2, a2 = activate(z2)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.activate(self.z1)\n",
    "        \n",
    "        # Layer 2 (output)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        if self.loss_function == 'binary_crossentropy':\n",
    "            self.a2 = self.activate(self.z2, 'sigmoid')  # Sigmoid for classification\n",
    "        else:\n",
    "            self.a2 = self.z2  # Linear for regression\n",
    "            \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute loss\"\"\"\n",
    "        if self.loss_function == 'mse':\n",
    "            return np.mean((y_true - y_pred)**2)\n",
    "        elif self.loss_function == 'binary_crossentropy':\n",
    "            y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "            return -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        if self.loss_function == 'binary_crossentropy':\n",
    "            dz2 = self.a2 - y\n",
    "        else:\n",
    "            dz2 = 2 * (self.a2 - y) / m\n",
    "        \n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.activate_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=False):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, predictions)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"‚úÖ Neural Network class implemented!\")\n",
    "print(\"Now let's test it on some data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Activation Function Comparison (3 minutes)\n",
    "\n",
    "Let's see how different activation functions perform on the same problem.\n",
    "\n",
    "### üéØ Your Task:\n",
    "Run the code below and observe how different activation functions affect learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, n_clusters_per_class=1, \n",
    "                          random_state=42)\n",
    "y = y.reshape(-1, 1)  # Reshape for our network\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_scaled.shape[0]} samples\")\n",
    "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "# Test different activation functions\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "networks = {}\n",
    "results = {}\n",
    "\n",
    "print(\"\\nüß™ Training networks with different activation functions...\")\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"\\nTraining with {activation.upper()} activation:\")\n",
    "    \n",
    "    # Create and train network\n",
    "    network = SimpleNeuralNetwork(input_size=2, hidden_size=10, output_size=1, \n",
    "                                 activation=activation, loss='binary_crossentropy')\n",
    "    \n",
    "    network.train(X_train_scaled, y_train, epochs=500, learning_rate=0.1, verbose=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = network.predict(X_train_scaled)\n",
    "    test_pred = network.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_acc = np.mean((train_pred > 0.5) == y_train)\n",
    "    test_acc = np.mean((test_pred > 0.5) == y_test)\n",
    "    \n",
    "    networks[activation] = network\n",
    "    results[activation] = {'train_acc': train_acc, 'test_acc': test_acc}\n",
    "    \n",
    "    print(f\"Final - Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä ACTIVATION FUNCTION COMPARISON RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "for activation in activations:\n",
    "    train_acc = results[activation]['train_acc']\n",
    "    test_acc = results[activation]['test_acc']\n",
    "    print(f\"{activation.upper():8s}: Train={train_acc:.3f}, Test={test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "ax1 = axes[0, 0]\n",
    "for activation in activations:\n",
    "    network = networks[activation]\n",
    "    ax1.plot(network.losses, label=f'{activation.upper()}', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Learning Curves: Different Activation Functions', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "ax2 = axes[0, 1]\n",
    "train_accs = [results[act]['train_acc'] for act in activations]\n",
    "test_accs = [results[act]['test_acc'] for act in activations]\n",
    "\n",
    "x = np.arange(len(activations))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, train_accs, width, label='Train Accuracy', alpha=0.8)\n",
    "ax2.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Final Accuracy Comparison', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([act.upper() for act in activations])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3 & 4: Decision boundaries for best and worst performing models\n",
    "best_activation = max(activations, key=lambda x: results[x]['test_acc'])\n",
    "worst_activation = min(activations, key=lambda x: results[x]['test_acc'])\n",
    "\n",
    "for i, (activation, title) in enumerate([(best_activation, 'Best Performer'), \n",
    "                                        (worst_activation, 'Worst Performer')]):\n",
    "    ax = axes[1, i]\n",
    "    network = networks[activation]\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = network.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "    scatter = ax.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], \n",
    "                        c=y_test.ravel(), cmap='RdYlBu', edgecolors='black')\n",
    "    \n",
    "    test_acc = results[activation]['test_acc']\n",
    "    ax.set_title(f'{title}: {activation.upper()}\\nTest Accuracy: {test_acc:.3f}', \n",
    "                fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Best performing activation: {best_activation.upper()}\")\n",
    "print(f\"üîß Try experimenting with different network architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Loss Function Impact (2 minutes)\n",
    "\n",
    "Let's see how different loss functions affect regression performance.\n",
    "\n",
    "### üéØ Your Task:\n",
    "Observe how MSE vs MAE loss functions handle outliers differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data with outliers\n",
    "np.random.seed(42)\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(len(y_reg), size=20, replace=False)\n",
    "y_reg[outlier_indices] += np.random.normal(0, 50, 20)  # Add large noise to create outliers\n",
    "\n",
    "y_reg = y_reg.reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_reg_scaled = scaler_X.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_X.transform(X_test_reg)\n",
    "y_train_reg_scaled = scaler_y.fit_transform(y_train_reg)\n",
    "y_test_reg_scaled = scaler_y.transform(y_test_reg)\n",
    "\n",
    "print(\"üéØ Training regression models with different loss functions...\")\n",
    "\n",
    "# We'll simulate MAE by modifying our loss function\n",
    "class MAENeuralNetwork(SimpleNeuralNetwork):\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # MAE gradient\n",
    "        dz2 = np.sign(self.a2 - y) / m\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.activate_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "# Train with MSE\n",
    "print(\"\\nTraining with MSE loss:\")\n",
    "mse_network = SimpleNeuralNetwork(input_size=1, hidden_size=20, output_size=1, \n",
    "                                 activation='relu', loss='mse')\n",
    "mse_network.train(X_train_reg_scaled, y_train_reg_scaled, epochs=1000, \n",
    "                 learning_rate=0.01, verbose=True)\n",
    "\n",
    "# Train with MAE\n",
    "print(\"\\nTraining with MAE loss:\")\n",
    "mae_network = MAENeuralNetwork(input_size=1, hidden_size=20, output_size=1, \n",
    "                              activation='relu', loss='mse')  # We override the loss anyway\n",
    "mae_network.train(X_train_reg_scaled, y_train_reg_scaled, epochs=1000, \n",
    "                 learning_rate=0.01, verbose=True)\n",
    "\n",
    "# Make predictions\n",
    "mse_pred_train = mse_network.predict(X_train_reg_scaled)\n",
    "mse_pred_test = mse_network.predict(X_test_reg_scaled)\n",
    "mae_pred_train = mae_network.predict(X_train_reg_scaled)\n",
    "mae_pred_test = mae_network.predict(X_test_reg_scaled)\n",
    "\n",
    "# Convert back to original scale\n",
    "mse_pred_train_orig = scaler_y.inverse_transform(mse_pred_train)\n",
    "mse_pred_test_orig = scaler_y.inverse_transform(mse_pred_test)\n",
    "mae_pred_train_orig = scaler_y.inverse_transform(mae_pred_train)\n",
    "mae_pred_test_orig = scaler_y.inverse_transform(mae_pred_test)\n",
    "\n",
    "y_train_orig = scaler_y.inverse_transform(y_train_reg_scaled)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_reg_scaled)\n",
    "\n",
    "print(\"\\nüìä Regression Results:\")\n",
    "print(f\"MSE Network - Train MSE: {np.mean((y_train_orig - mse_pred_train_orig)**2):.2f}\")\n",
    "print(f\"MSE Network - Test MSE: {np.mean((y_test_orig - mse_pred_test_orig)**2):.2f}\")\n",
    "print(f\"MAE Network - Train MSE: {np.mean((y_train_orig - mae_pred_train_orig)**2):.2f}\")\n",
    "print(f\"MAE Network - Test MSE: {np.mean((y_test_orig - mae_pred_test_orig)**2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(mse_network.losses, 'b-', label='MSE Loss', linewidth=2)\n",
    "ax1.plot(mae_network.losses, 'r-', label='MAE Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Learning Curves: MSE vs MAE', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Training data predictions\n",
    "ax2 = axes[0, 1]\n",
    "sort_idx = np.argsort(X_train_reg.ravel())\n",
    "ax2.scatter(X_train_reg, y_train_orig, alpha=0.6, label='True Data', color='gray')\n",
    "ax2.plot(X_train_reg[sort_idx], mse_pred_train_orig[sort_idx], 'b-', \n",
    "         label='MSE Prediction', linewidth=2)\n",
    "ax2.plot(X_train_reg[sort_idx], mae_pred_train_orig[sort_idx], 'r-', \n",
    "         label='MAE Prediction', linewidth=2)\n",
    "ax2.set_xlabel('Input Feature')\n",
    "ax2.set_ylabel('Target Value')\n",
    "ax2.set_title('Training Data Predictions', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Test data predictions\n",
    "ax3 = axes[1, 0]\n",
    "sort_idx_test = np.argsort(X_test_reg.ravel())\n",
    "ax3.scatter(X_test_reg, y_test_orig, alpha=0.6, label='True Data', color='gray')\n",
    "ax3.plot(X_test_reg[sort_idx_test], mse_pred_test_orig[sort_idx_test], 'b-', \n",
    "         label='MSE Prediction', linewidth=2)\n",
    "ax3.plot(X_test_reg[sort_idx_test], mae_pred_test_orig[sort_idx_test], 'r-', \n",
    "         label='MAE Prediction', linewidth=2)\n",
    "ax3.set_xlabel('Input Feature')\n",
    "ax3.set_ylabel('Target Value')\n",
    "ax3.set_title('Test Data Predictions', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Error analysis\n",
    "ax4 = axes[1, 1]\n",
    "mse_errors = np.abs(y_test_orig.ravel() - mse_pred_test_orig.ravel())\n",
    "mae_errors = np.abs(y_test_orig.ravel() - mae_pred_test_orig.ravel())\n",
    "\n",
    "ax4.scatter(range(len(mse_errors)), mse_errors, alpha=0.7, label='MSE Model Errors')\n",
    "ax4.scatter(range(len(mae_errors)), mae_errors, alpha=0.7, label='MAE Model Errors')\n",
    "ax4.axhline(y=np.mean(mse_errors), color='blue', linestyle='--', \n",
    "           label=f'MSE Mean Error: {np.mean(mse_errors):.1f}')\n",
    "ax4.axhline(y=np.mean(mae_errors), color='red', linestyle='--', \n",
    "           label=f'MAE Mean Error: {np.mean(mae_errors):.1f}')\n",
    "ax4.set_xlabel('Test Sample')\n",
    "ax4.set_ylabel('Absolute Error')\n",
    "ax4.set_title('Error Analysis', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"‚Ä¢ MSE loss is more sensitive to outliers (larger errors get squared)\")\n",
    "print(\"‚Ä¢ MAE loss treats all errors equally\")\n",
    "print(\"‚Ä¢ Choice of loss function affects how the model handles outliers\")\n",
    "print(\"‚Ä¢ This is why understanding your data is crucial for loss selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Build Your Own Experiment (2 minutes)\n",
    "\n",
    "Now it's your turn to experiment! Try modifying the code to explore different scenarios.\n",
    "\n",
    "### üéØ Your Challenges:\n",
    "Pick one or more of these experiments to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ EXPERIMENT CHALLENGES:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. üèóÔ∏è  ARCHITECTURE: Try different hidden layer sizes (5, 20, 50)\")\n",
    "print(\"2. üìö  LEARNING: Experiment with different learning rates (0.001, 0.01, 0.1)\")\n",
    "print(\"3. üéØ  DATA: Create a more complex dataset (more features, non-linear patterns)\")\n",
    "print(\"4. üîß  OPTIMIZATION: Try training for different numbers of epochs\")\n",
    "print(\"5. üé®  VISUALIZATION: Plot how decision boundaries change during training\")\n",
    "print()\n",
    "print(\"üí° BONUS CHALLENGE: Can you solve the XOR problem we discussed earlier?\")\n",
    "print(\"   Hint: XOR data looks like: [[0,0]‚Üí0, [0,1]‚Üí1, [1,0]‚Üí1, [1,1]‚Üí0]\")\n",
    "print()\n",
    "print(\"Choose one experiment and implement it below! üëá\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# YOUR EXPERIMENT CODE HERE!\n",
    "# Example: Solving XOR problem\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"\\nüî• BONUS: Solving the XOR Problem!\")\n",
    "print(\"Remember: The perceptron couldn't solve this, but our neural network can!\")\n",
    "\n",
    "# Train neural network on XOR\n",
    "xor_network = SimpleNeuralNetwork(input_size=2, hidden_size=10, output_size=1, \n",
    "                                 activation='relu', loss='binary_crossentropy')\n",
    "\n",
    "print(\"\\nTraining on XOR data...\")\n",
    "xor_network.train(X_xor, y_xor, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Test predictions\n",
    "xor_predictions = xor_network.predict(X_xor)\n",
    "xor_binary_pred = (xor_predictions > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nüéØ XOR Results:\")\n",
    "print(\"Input | True | Predicted | Probability | Correct?\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X_xor)):\n",
    "    input_str = f\"{X_xor[i]}\"\n",
    "    true_val = y_xor[i, 0]\n",
    "    pred_val = xor_binary_pred[i, 0]\n",
    "    prob_val = xor_predictions[i, 0]\n",
    "    correct = \"‚úÖ\" if pred_val == true_val else \"‚ùå\"\n",
    "    print(f\"{input_str:10s} | {true_val:4d} | {pred_val:9d} | {prob_val:11.3f} | {correct}\")\n",
    "\n",
    "accuracy = np.mean(xor_binary_pred == y_xor)\n",
    "print(f\"\\nüèÜ XOR Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    print(\"üéâ CONGRATULATIONS! You've solved the XOR problem!\")\n",
    "    print(\"üß† This is what the perceptron couldn't do - you've built something more powerful!\")\n",
    "else:\n",
    "    print(\"üîß Try adjusting the network architecture or training parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XOR solution\n",
    "if 'xor_network' in locals():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: XOR data and decision boundary\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.01\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = xor_network.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot XOR points\n",
    "    colors = ['red', 'blue', 'blue', 'red']\n",
    "    for i, (point, color, label) in enumerate(zip(X_xor, colors, y_xor.ravel())):\n",
    "        plt.scatter(point[0], point[1], c=color, s=200, edgecolors='black', linewidth=3)\n",
    "        plt.annotate(f'({point[0]},{point[1]})‚Üí{label}', \n",
    "                    (point[0], point[1]), \n",
    "                    xytext=(10, 10), textcoords='offset points', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.xlim(-0.3, 1.3)\n",
    "    plt.ylim(-0.3, 1.3)\n",
    "    plt.xlabel('Input 1', fontsize=12)\n",
    "    plt.ylabel('Input 2', fontsize=12)\n",
    "    plt.title('XOR Problem Solved!\\nNon-linear Decision Boundary', fontweight='bold')\n",
    "    plt.colorbar(label='Output Probability')\n",
    "    \n",
    "    # Plot 2: Learning curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(xor_network.losses, 'b-', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('XOR Learning Curve', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéì Notice how the neural network creates a non-linear decision boundary!\")\n",
    "    print(\"üî• This is the power of hidden layers + non-linear activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Workshop Wrap-up & Next Steps\n",
    "\n",
    "### Congratulations! You've Successfully:\n",
    "\n",
    "‚úÖ **Understood Supervised Learning** - The foundation of neural networks  \n",
    "‚úÖ **Implemented a Perceptron** - The first artificial neuron  \n",
    "‚úÖ **Explored Activation Functions** - The key to non-linearity  \n",
    "‚úÖ **Mastered Loss Functions** - How networks learn from mistakes  \n",
    "‚úÖ **Built a Complete Neural Network** - Putting it all together!  \n",
    "\n",
    "### üß† Key Insights You've Gained:\n",
    "\n",
    "1. **Neural networks are just mathematical functions** that learn to map inputs to outputs\n",
    "2. **Each component serves a purpose**: perceptrons process information, activations add non-linearity, loss functions guide learning\n",
    "3. **The magic happens when components work together** - no single part makes a neural network powerful\n",
    "4. **Understanding fundamentals helps you debug and improve** real-world models\n",
    "\n",
    "### üöÄ Where to Go From Here:\n",
    "\n",
    "#### Immediate Next Steps:\n",
    "- **Experiment more** with the code you've written\n",
    "- **Try different datasets** from sklearn.datasets\n",
    "- **Modify network architectures** (more layers, different sizes)\n",
    "- **Explore hyperparameter tuning** (learning rates, epochs)\n",
    "\n",
    "#### Advanced Topics to Explore:\n",
    "- **Convolutional Neural Networks (CNNs)** for image data\n",
    "- **Recurrent Neural Networks (RNNs)** for sequential data\n",
    "- **Deep Learning Frameworks** like TensorFlow, PyTorch\n",
    "- **Regularization Techniques** to prevent overfitting\n",
    "- **Modern Architectures** like Transformers, ResNets\n",
    "\n",
    "#### Resources for Continued Learning:\n",
    "- **Online Courses**: Coursera Deep Learning Specialization, Fast.ai\n",
    "- **Books**: \"Deep Learning\" by Goodfellow, \"Neural Networks and Deep Learning\" by Nielsen\n",
    "- **Practice Platforms**: Kaggle, Google Colab\n",
    "- **Communities**: Reddit r/MachineLearning, AI/ML Discord servers\n",
    "\n",
    "### üí° Remember:\n",
    "\n",
    "- **Start simple, then add complexity** - master the basics first\n",
    "- **Understand your data** before choosing architectures\n",
    "- **Experiment and iterate** - ML is empirical\n",
    "- **Focus on problems you care about** - motivation drives learning\n",
    "\n",
    "### üéØ Final Challenge:\n",
    "\n",
    "Take a problem from your domain and try to formulate it as a supervised learning task:\n",
    "- What are your inputs and outputs?\n",
    "- Is it classification or regression?\n",
    "- What activation and loss functions would you choose?\n",
    "- How would you evaluate success?\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Workshop Resources\n",
    "\n",
    "### Code Repository:\n",
    "- All notebooks are available on GitHub\n",
    "- Each notebook can be opened directly in Google Colab\n",
    "- Feel free to fork, modify, and share!\n",
    "\n",
    "### Contact & Support:\n",
    "- Workshop materials: [GitHub Repository](https://github.com/maleehahassan/NNBuildingBlocksTeachingPt1)\n",
    "- Questions? Open an issue in the repository\n",
    "- Want to contribute? Pull requests welcome!\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for participating in the Neural Networks Building Blocks Workshop!** üéì\n",
    "\n",
    "*Remember: Every expert was once a beginner. Keep learning, keep experimenting, and most importantly, keep building!* üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}