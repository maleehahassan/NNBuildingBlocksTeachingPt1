{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hands-on Exercises\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maleehahassan/NNBuildingBlocksTeachingPt1/blob/main/content/05_hands_on_exercises.ipynb)\n",
    "\n",
    "## Workshop Summary & Practice\n",
    "\n",
    "Congratulations! You've learned about the fundamental building blocks of neural networks:\n",
    "\n",
    "1. **Supervised Learning** - Learning from labeled examples\n",
    "2. **Perceptron** - The first artificial neuron and its limitations\n",
    "3. **Activation Functions** - Adding non-linearity to enable complex patterns\n",
    "4. **Loss Functions** - Measuring and minimizing prediction errors\n",
    "\n",
    "Now it's time to **put it all together** with hands-on exercises!\n",
    "\n",
    "## Exercise Overview\n",
    "\n",
    "In the next 10 minutes, you'll:\n",
    "- Build a simple neural network from scratch\n",
    "- Experiment with different activation functions\n",
    "- Compare different loss functions\n",
    "- See how all the concepts work together\n",
    "\n",
    "### 🎯 Learning Objectives\n",
    "- Apply the concepts you've learned\n",
    "- Build intuition through experimentation\n",
    "- Understand how components interact\n",
    "- Gain confidence in neural network basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Setup - Import required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification, make_regression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Setup - Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"🚀 Welcome to the Neural Networks Building Blocks Workshop!\")\n",
    "print(\"Let's put everything we've learned into practice!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build Your First Neural Network (3 minutes)\n",
    "\n",
    "Let's build a simple neural network class that incorporates all the concepts we've learned.\n",
    "\n",
    "### 🎯 Your Task:\n",
    "Complete the missing parts in the neural network implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu', loss='mse'):\n",
    "        \"\"\"\n",
    "        Initialize a simple 2-layer neural network\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: number of input features\n",
    "        - hidden_size: number of neurons in hidden layer\n",
    "        - output_size: number of output neurons\n",
    "        - activation: activation function ('relu', 'sigmoid', 'tanh')\n",
    "        - loss: loss function ('mse', 'binary_crossentropy')\n",
    "        \"\"\"\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.loss_function = loss\n",
    "        self.losses = []\n",
    "    \n",
    "    def activate(self, z, activation=None):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if activation is None:\n",
    "            activation = self.activation\n",
    "            \n",
    "        if activation == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        elif activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "    def activate_derivative(self, z, activation=None):\n",
    "        \"\"\"Compute derivative of activation function\"\"\"\n",
    "        if activation is None:\n",
    "            activation = self.activation\n",
    "            \n",
    "        if activation == 'relu':\n",
    "            return (z > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            s = self.activate(z, 'sigmoid')\n",
    "            return s * (1 - s)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - np.tanh(z)**2\n",
    "        else:\n",
    "            return np.ones_like(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: z1 = X @ W1 + b1, a1 = activate(z1), z2 = a1 @ W2 + b2, a2 = activate(z2)\n",
    "        \n",
    "        # Layer 1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.activate(self.z1)\n",
    "        \n",
    "        # Layer 2 (output)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        if self.loss_function == 'binary_crossentropy':\n",
    "            self.a2 = self.activate(self.z2, 'sigmoid')  # Sigmoid for classification\n",
    "        else:\n",
    "            self.a2 = self.z2  # Linear for regression\n",
    "            \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute loss\"\"\"\n",
    "        if self.loss_function == 'mse':\n",
    "            return np.mean((y_true - y_pred)**2)\n",
    "        elif self.loss_function == 'binary_crossentropy':\n",
    "            y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "            return -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        if self.loss_function == 'binary_crossentropy':\n",
    "            dz2 = self.a2 - y\n",
    "        else:\n",
    "            dz2 = 2 * (self.a2 - y) / m\n",
    "        \n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.activate_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=False):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, predictions)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"✅ Neural Network class implemented!\")\n",
    "print(\"Now let's test it on some data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Activation Function Comparison (3 minutes)\n",
    "\n",
    "Let's see how different activation functions perform on the same problem.\n",
    "\n",
    "### 🎯 Your Task:\n",
    "Run the code below and observe how different activation functions affect learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, n_clusters_per_class=1, \n",
    "                          random_state=42)\n",
    "y = y.reshape(-1, 1)  # Reshape for our network\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_scaled.shape[0]} samples\")\n",
    "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "# Test different activation functions\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "networks = {}\n",
    "results = {}\n",
    "\n",
    "print(\"\\n🧪 Training networks with different activation functions...\")\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"\\nTraining with {activation.upper()} activation:\")\n",
    "    \n",
    "    # Create and train network\n",
    "    network = SimpleNeuralNetwork(input_size=2, hidden_size=10, output_size=1, \n",
    "                                 activation=activation, loss='binary_crossentropy')\n",
    "    \n",
    "    network.train(X_train_scaled, y_train, epochs=500, learning_rate=0.1, verbose=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = network.predict(X_train_scaled)\n",
    "    test_pred = network.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_acc = np.mean((train_pred > 0.5) == y_train)\n",
    "    test_acc = np.mean((test_pred > 0.5) == y_test)\n",
    "    \n",
    "    networks[activation] = network\n",
    "    results[activation] = {'train_acc': train_acc, 'test_acc': test_acc}\n",
    "    \n",
    "    print(f\"Final - Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 ACTIVATION FUNCTION COMPARISON RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "for activation in activations:\n",
    "    train_acc = results[activation]['train_acc']\n",
    "    test_acc = results[activation]['test_acc']\n",
    "    print(f\"{activation.upper():8s}: Train={train_acc:.3f}, Test={test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "ax1 = axes[0, 0]\n",
    "for activation in activations:\n",
    "    network = networks[activation]\n",
    "    ax1.plot(network.losses, label=f'{activation.upper()}', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Learning Curves: Different Activation Functions', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "ax2 = axes[0, 1]\n",
    "train_accs = [results[act]['train_acc'] for act in activations]\n",
    "test_accs = [results[act]['test_acc'] for act in activations]\n",
    "\n",
    "x = np.arange(len(activations))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, train_accs, width, label='Train Accuracy', alpha=0.8)\n",
    "ax2.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Final Accuracy Comparison', fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([act.upper() for act in activations])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3 & 4: Decision boundaries for best and worst performing models\n",
    "best_activation = max(activations, key=lambda x: results[x]['test_acc'])\n",
    "worst_activation = min(activations, key=lambda x: results[x]['test_acc'])\n",
    "\n",
    "for i, (activation, title) in enumerate([(best_activation, 'Best Performer'), \n",
    "                                        (worst_activation, 'Worst Performer')]):\n",
    "    ax = axes[1, i]\n",
    "    network = networks[activation]\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = network.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "    scatter = ax.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], \n",
    "                        c=y_test.ravel(), cmap='RdYlBu', edgecolors='black')\n",
    "    \n",
    "    test_acc = results[activation]['test_acc']\n",
    "    ax.set_title(f'{title}: {activation.upper()}\\nTest Accuracy: {test_acc:.3f}', \n",
    "                fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🏆 Best performing activation: {best_activation.upper()}\")\n",
    "print(f\"🔧 Try experimenting with different network architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Loss Function Impact (2 minutes)\n",
    "\n",
    "Let's see how different loss functions affect regression performance.\n",
    "\n",
    "### 🎯 Your Task:\n",
    "Observe how MSE vs MAE loss functions handle outliers differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data with outliers\n",
    "np.random.seed(42)\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(len(y_reg), size=20, replace=False)\n",
    "y_reg[outlier_indices] += np.random.normal(0, 50, 20)  # Add large noise to create outliers\n",
    "\n",
    "y_reg = y_reg.reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_reg_scaled = scaler_X.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_X.transform(X_test_reg)\n",
    "y_train_reg_scaled = scaler_y.fit_transform(y_train_reg)\n",
    "y_test_reg_scaled = scaler_y.transform(y_test_reg)\n",
    "\n",
    "print(\"🎯 Training regression models with different loss functions...\")\n",
    "\n",
    "# We'll simulate MAE by modifying our loss function\n",
    "class MAENeuralNetwork(SimpleNeuralNetwork):\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # MAE gradient\n",
    "        dz2 = np.sign(self.a2 - y) / m\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.activate_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "# Train with MSE\n",
    "print(\"\\nTraining with MSE loss:\")\n",
    "mse_network = SimpleNeuralNetwork(input_size=1, hidden_size=20, output_size=1, \n",
    "                                 activation='relu', loss='mse')\n",
    "mse_network.train(X_train_reg_scaled, y_train_reg_scaled, epochs=1000, \n",
    "                 learning_rate=0.01, verbose=True)\n",
    "\n",
    "# Train with MAE\n",
    "print(\"\\nTraining with MAE loss:\")\n",
    "mae_network = MAENeuralNetwork(input_size=1, hidden_size=20, output_size=1, \n",
    "                              activation='relu', loss='mse')  # We override the loss anyway\n",
    "mae_network.train(X_train_reg_scaled, y_train_reg_scaled, epochs=1000, \n",
    "                 learning_rate=0.01, verbose=True)\n",
    "\n",
    "# Make predictions\n",
    "mse_pred_train = mse_network.predict(X_train_reg_scaled)\n",
    "mse_pred_test = mse_network.predict(X_test_reg_scaled)\n",
    "mae_pred_train = mae_network.predict(X_train_reg_scaled)\n",
    "mae_pred_test = mae_network.predict(X_test_reg_scaled)\n",
    "\n",
    "# Convert back to original scale\n",
    "mse_pred_train_orig = scaler_y.inverse_transform(mse_pred_train)\n",
    "mse_pred_test_orig = scaler_y.inverse_transform(mse_pred_test)\n",
    "mae_pred_train_orig = scaler_y.inverse_transform(mae_pred_train)\n",
    "mae_pred_test_orig = scaler_y.inverse_transform(mae_pred_test)\n",
    "\n",
    "y_train_orig = scaler_y.inverse_transform(y_train_reg_scaled)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_reg_scaled)\n",
    "\n",
    "print(\"\\n📊 Regression Results:\")\n",
    "print(f\"MSE Network - Train MSE: {np.mean((y_train_orig - mse_pred_train_orig)**2):.2f}\")\n",
    "print(f\"MSE Network - Test MSE: {np.mean((y_test_orig - mse_pred_test_orig)**2):.2f}\")\n",
    "print(f\"MAE Network - Train MSE: {np.mean((y_train_orig - mae_pred_train_orig)**2):.2f}\")\n",
    "print(f\"MAE Network - Test MSE: {np.mean((y_test_orig - mae_pred_test_orig)**2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(mse_network.losses, 'b-', label='MSE Loss', linewidth=2)\n",
    "ax1.plot(mae_network.losses, 'r-', label='MAE Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Learning Curves: MSE vs MAE', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Training data predictions\n",
    "ax2 = axes[0, 1]\n",
    "sort_idx = np.argsort(X_train_reg.ravel())\n",
    "ax2.scatter(X_train_reg, y_train_orig, alpha=0.6, label='True Data', color='gray')\n",
    "ax2.plot(X_train_reg[sort_idx], mse_pred_train_orig[sort_idx], 'b-', \n",
    "         label='MSE Prediction', linewidth=2)\n",
    "ax2.plot(X_train_reg[sort_idx], mae_pred_train_orig[sort_idx], 'r-', \n",
    "         label='MAE Prediction', linewidth=2)\n",
    "ax2.set_xlabel('Input Feature')\n",
    "ax2.set_ylabel('Target Value')\n",
    "ax2.set_title('Training Data Predictions', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Test data predictions\n",
    "ax3 = axes[1, 0]\n",
    "sort_idx_test = np.argsort(X_test_reg.ravel())\n",
    "ax3.scatter(X_test_reg, y_test_orig, alpha=0.6, label='True Data', color='gray')\n",
    "ax3.plot(X_test_reg[sort_idx_test], mse_pred_test_orig[sort_idx_test], 'b-', \n",
    "         label='MSE Prediction', linewidth=2)\n",
    "ax3.plot(X_test_reg[sort_idx_test], mae_pred_test_orig[sort_idx_test], 'r-', \n",
    "         label='MAE Prediction', linewidth=2)\n",
    "ax3.set_xlabel('Input Feature')\n",
    "ax3.set_ylabel('Target Value')\n",
    "ax3.set_title('Test Data Predictions', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Error analysis\n",
    "ax4 = axes[1, 1]\n",
    "mse_errors = np.abs(y_test_orig.ravel() - mse_pred_test_orig.ravel())\n",
    "mae_errors = np.abs(y_test_orig.ravel() - mae_pred_test_orig.ravel())\n",
    "\n",
    "ax4.scatter(range(len(mse_errors)), mse_errors, alpha=0.7, label='MSE Model Errors')\n",
    "ax4.scatter(range(len(mae_errors)), mae_errors, alpha=0.7, label='MAE Model Errors')\n",
    "ax4.axhline(y=np.mean(mse_errors), color='blue', linestyle='--', \n",
    "           label=f'MSE Mean Error: {np.mean(mse_errors):.1f}')\n",
    "ax4.axhline(y=np.mean(mae_errors), color='red', linestyle='--', \n",
    "           label=f'MAE Mean Error: {np.mean(mae_errors):.1f}')\n",
    "ax4.set_xlabel('Test Sample')\n",
    "ax4.set_ylabel('Absolute Error')\n",
    "ax4.set_title('Error Analysis', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Key Observations:\")\n",
    "print(\"• MSE loss is more sensitive to outliers (larger errors get squared)\")\n",
    "print(\"• MAE loss treats all errors equally\")\n",
    "print(\"• Choice of loss function affects how the model handles outliers\")\n",
    "print(\"• This is why understanding your data is crucial for loss selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Build Your Own Experiment (2 minutes)\n",
    "\n",
    "Now it's your turn to experiment! Try modifying the code to explore different scenarios.\n",
    "\n",
    "### 🎯 Your Challenges:\n",
    "Pick one or more of these experiments to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 EXPERIMENT CHALLENGES:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. 🏗️  ARCHITECTURE: Try different hidden layer sizes (5, 20, 50)\")\n",
    "print(\"2. 📚  LEARNING: Experiment with different learning rates (0.001, 0.01, 0.1)\")\n",
    "print(\"3. 🎯  DATA: Create a more complex dataset (more features, non-linear patterns)\")\n",
    "print(\"4. 🔧  OPTIMIZATION: Try training for different numbers of epochs\")\n",
    "print(\"5. 🎨  VISUALIZATION: Plot how decision boundaries change during training\")\n",
    "print()\n",
    "print(\"💡 BONUS CHALLENGE: Can you solve the XOR problem we discussed earlier?\")\n",
    "print(\"   Hint: XOR data looks like: [[0,0]→0, [0,1]→1, [1,0]→1, [1,1]→0]\")\n",
    "print()\n",
    "print(\"Choose one experiment and implement it below! 👇\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# YOUR EXPERIMENT CODE HERE!\n",
    "# Example: Solving XOR problem\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"\\n🔥 BONUS: Solving the XOR Problem!\")\n",
    "print(\"Remember: The perceptron couldn't solve this, but our neural network can!\")\n",
    "\n",
    "# Train neural network on XOR\n",
    "xor_network = SimpleNeuralNetwork(input_size=2, hidden_size=10, output_size=1, \n",
    "                                 activation='relu', loss='binary_crossentropy')\n",
    "\n",
    "print(\"\\nTraining on XOR data...\")\n",
    "xor_network.train(X_xor, y_xor, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Test predictions\n",
    "xor_predictions = xor_network.predict(X_xor)\n",
    "xor_binary_pred = (xor_predictions > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n🎯 XOR Results:\")\n",
    "print(\"Input | True | Predicted | Probability | Correct?\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X_xor)):\n",
    "    input_str = f\"{X_xor[i]}\"\n",
    "    true_val = y_xor[i, 0]\n",
    "    pred_val = xor_binary_pred[i, 0]\n",
    "    prob_val = xor_predictions[i, 0]\n",
    "    correct = \"✅\" if pred_val == true_val else \"❌\"\n",
    "    print(f\"{input_str:10s} | {true_val:4d} | {pred_val:9d} | {prob_val:11.3f} | {correct}\")\n",
    "\n",
    "accuracy = np.mean(xor_binary_pred == y_xor)\n",
    "print(f\"\\n🏆 XOR Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    print(\"🎉 CONGRATULATIONS! You've solved the XOR problem!\")\n",
    "    print(\"🧠 This is what the perceptron couldn't do - you've built something more powerful!\")\n",
    "else:\n",
    "    print(\"🔧 Try adjusting the network architecture or training parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XOR solution\n",
    "if 'xor_network' in locals():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: XOR data and decision boundary\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.01\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = xor_network.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot XOR points\n",
    "    colors = ['red', 'blue', 'blue', 'red']\n",
    "    for i, (point, color, label) in enumerate(zip(X_xor, colors, y_xor.ravel())):\n",
    "        plt.scatter(point[0], point[1], c=color, s=200, edgecolors='black', linewidth=3)\n",
    "        plt.annotate(f'({point[0]},{point[1]})→{label}', \n",
    "                    (point[0], point[1]), \n",
    "                    xytext=(10, 10), textcoords='offset points', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.xlim(-0.3, 1.3)\n",
    "    plt.ylim(-0.3, 1.3)\n",
    "    plt.xlabel('Input 1', fontsize=12)\n",
    "    plt.ylabel('Input 2', fontsize=12)\n",
    "    plt.title('XOR Problem Solved!\\nNon-linear Decision Boundary', fontweight='bold')\n",
    "    plt.colorbar(label='Output Probability')\n",
    "    \n",
    "    # Plot 2: Learning curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(xor_network.losses, 'b-', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('XOR Learning Curve', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🎓 Notice how the neural network creates a non-linear decision boundary!\")\n",
    "    print(\"🔥 This is the power of hidden layers + non-linear activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Workshop Wrap-up & Next Steps\n",
    "\n",
    "### Congratulations! You've Successfully:\n",
    "\n",
    "✅ **Understood Supervised Learning** - The foundation of neural networks  \n",
    "✅ **Implemented a Perceptron** - The first artificial neuron  \n",
    "✅ **Explored Activation Functions** - The key to non-linearity  \n",
    "✅ **Mastered Loss Functions** - How networks learn from mistakes  \n",
    "✅ **Built a Complete Neural Network** - Putting it all together!  \n",
    "\n",
    "### 🧠 Key Insights You've Gained:\n",
    "\n",
    "1. **Neural networks are just mathematical functions** that learn to map inputs to outputs\n",
    "2. **Each component serves a purpose**: perceptrons process information, activations add non-linearity, loss functions guide learning\n",
    "3. **The magic happens when components work together** - no single part makes a neural network powerful\n",
    "4. **Understanding fundamentals helps you debug and improve** real-world models\n",
    "\n",
    "### 🚀 Where to Go From Here:\n",
    "\n",
    "#### Immediate Next Steps:\n",
    "- **Experiment more** with the code you've written\n",
    "- **Try different datasets** from sklearn.datasets\n",
    "- **Modify network architectures** (more layers, different sizes)\n",
    "- **Explore hyperparameter tuning** (learning rates, epochs)\n",
    "\n",
    "#### Advanced Topics to Explore:\n",
    "- **Convolutional Neural Networks (CNNs)** for image data\n",
    "- **Recurrent Neural Networks (RNNs)** for sequential data\n",
    "- **Deep Learning Frameworks** like TensorFlow, PyTorch\n",
    "- **Regularization Techniques** to prevent overfitting\n",
    "- **Modern Architectures** like Transformers, ResNets\n",
    "\n",
    "#### Resources for Continued Learning:\n",
    "- **Online Courses**: Coursera Deep Learning Specialization, Fast.ai\n",
    "- **Books**: \"Deep Learning\" by Goodfellow, \"Neural Networks and Deep Learning\" by Nielsen\n",
    "- **Practice Platforms**: Kaggle, Google Colab\n",
    "- **Communities**: Reddit r/MachineLearning, AI/ML Discord servers\n",
    "\n",
    "### 💡 Remember:\n",
    "\n",
    "- **Start simple, then add complexity** - master the basics first\n",
    "- **Understand your data** before choosing architectures\n",
    "- **Experiment and iterate** - ML is empirical\n",
    "- **Focus on problems you care about** - motivation drives learning\n",
    "\n",
    "### 🎯 Final Challenge:\n",
    "\n",
    "Take a problem from your domain and try to formulate it as a supervised learning task:\n",
    "- What are your inputs and outputs?\n",
    "- Is it classification or regression?\n",
    "- What activation and loss functions would you choose?\n",
    "- How would you evaluate success?\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Workshop Resources\n",
    "\n",
    "### Code Repository:\n",
    "- All notebooks are available on GitHub\n",
    "- Each notebook can be opened directly in Google Colab\n",
    "- Feel free to fork, modify, and share!\n",
    "\n",
    "### Contact & Support:\n",
    "- Workshop materials: [GitHub Repository](https://github.com/maleehahassan/NNBuildingBlocksTeachingPt1)\n",
    "- Questions? Open an issue in the repository\n",
    "- Want to contribute? Pull requests welcome!\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for participating in the Neural Networks Building Blocks Workshop!** 🎓\n",
    "\n",
    "*Remember: Every expert was once a beginner. Keep learning, keep experimenting, and most importantly, keep building!* 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}