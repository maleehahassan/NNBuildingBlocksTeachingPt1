{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The First Perceptron\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maleehahassan/NNBuildingBlocksTeachingPt1/blob/main/content/02_first_perceptron.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you will understand:\n",
    "- The historical context and motivation behind the perceptron\n",
    "- How the perceptron works mathematically\n",
    "- The relationship between biological neurons and artificial neurons\n",
    "- Limitations and capabilities of the perceptron\n",
    "- How to implement a simple perceptron\n",
    "\n",
    "## Historical Context\n",
    "\n",
    "### The Birth of Artificial Neural Networks (1943-1958)\n",
    "\n",
    "- **1943**: McCulloch and Pitts proposed the first mathematical model of a neuron\n",
    "- **1949**: Donald Hebb introduced Hebbian learning (\"neurons that fire together, wire together\")\n",
    "- **1958**: Frank Rosenblatt invented the **Perceptron** at Cornell University\n",
    "\n",
    "The perceptron was revolutionary because it was the first algorithm that could **learn** to classify patterns automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Let's start by visualizing a biological neuron vs artificial neuron\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Circle, FancyBboxPatch\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Let's start by visualizing a biological neuron vs artificial neuron\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Circle, FancyBboxPatch\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Biological Neuron (simplified)\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 6)\n",
    "ax1.set_title('Biological Neuron', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Cell body\n",
    "circle = Circle((2, 3), 0.8, color='lightblue', alpha=0.7)\n",
    "ax1.add_patch(circle)\n",
    "ax1.text(2, 3, 'Cell\\nBody', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Dendrites (inputs)\n",
    "for i, y in enumerate([1.5, 2.5, 3.5, 4.5]):\n",
    "    ax1.arrow(0.2, y, 1.0, 0, head_width=0.1, head_length=0.1, fc='green', ec='green')\n",
    "    ax1.text(0.1, y, f'Input {i+1}', ha='right', va='center', fontsize=9)\n",
    "\n",
    "# Axon (output)\n",
    "ax1.arrow(2.8, 3, 6.5, 0, head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=2)\n",
    "ax1.text(9.5, 3, 'Output', ha='left', va='center', fontsize=10)\n",
    "\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Artificial Neuron (Perceptron)\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 6)\n",
    "ax2.set_title('Artificial Neuron (Perceptron)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Processing unit\n",
    "circle = Circle((5, 3), 1, color='lightcoral', alpha=0.7)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(5, 3, 'Σ', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Inputs with weights\n",
    "inputs = ['x₁', 'x₂', 'x₃']\n",
    "weights = ['w₁', 'w₂', 'w₃']\n",
    "for i, (inp, w) in enumerate(zip(inputs, weights)):\n",
    "    y = 2 + i * 0.7\n",
    "    ax2.arrow(1, y, 2.8, 3-y, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "    ax2.text(0.8, y, inp, ha='right', va='center', fontsize=12)\n",
    "    ax2.text(2.5, y+0.2, w, ha='center', va='center', fontsize=10, color='blue')\n",
    "\n",
    "# Bias\n",
    "ax2.arrow(5, 1, 0, 1.8, head_width=0.1, head_length=0.1, fc='purple', ec='purple')\n",
    "ax2.text(5, 0.8, 'bias', ha='center', va='center', fontsize=10, color='purple')\n",
    "\n",
    "# Output\n",
    "ax2.arrow(6, 3, 2.5, 0, head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=2)\n",
    "ax2.text(8.8, 3, 'y', ha='left', va='center', fontsize=12)\n",
    "\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Similarities:\")\n",
    "print(\"• Both receive multiple inputs\")\n",
    "print(\"• Both process and integrate information\")\n",
    "print(\"• Both produce a single output\")\n",
    "print(\"• Both can learn and adapt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Perceptron Works\n",
    "\n",
    "### The Mathematical Model\n",
    "\n",
    "A perceptron takes multiple inputs and produces a binary output (0 or 1). Here's how:\n",
    "\n",
    "1. **Inputs**: $x_1, x_2, ..., x_n$ (features)\n",
    "2. **Weights**: $w_1, w_2, ..., w_n$ (learned parameters)\n",
    "3. **Bias**: $b$ (threshold adjustment)\n",
    "4. **Weighted Sum**: $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n",
    "5. **Activation**: $y = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$\n",
    "\n",
    "### In Vector Form:\n",
    "- $z = \\mathbf{w}^T\\mathbf{x} + b$\n",
    "- $y = \\text{step}(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a simple perceptron step by step\n",
    "def step_function(z):\n",
    "    \"\"\"Step activation function\"\"\"\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "def perceptron_output(inputs, weights, bias):\n",
    "    \"\"\"Calculate perceptron output\"\"\"\n",
    "    # Calculate weighted sum\n",
    "    z = np.dot(weights, inputs) + bias\n",
    "    \n",
    "    # Apply step function\n",
    "    output = step_function(z)\n",
    "    \n",
    "    return z, output\n",
    "\n",
    "# Example: Simple AND gate\n",
    "print(\"=== Perceptron as AND Gate ===\")\n",
    "print(\"Trying to learn: output = 1 only when BOTH inputs are 1\")\n",
    "print()\n",
    "\n",
    "# Weights and bias for AND gate\n",
    "weights = np.array([0.5, 0.5])  # Equal importance to both inputs\n",
    "bias = -0.7  # High threshold\n",
    "\n",
    "# Test all possible inputs\n",
    "test_inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "expected_outputs = [0, 0, 0, 1]  # AND gate truth table\n",
    "\n",
    "print(\"Input 1 | Input 2 | Weighted Sum | Output | Expected\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    z, output = perceptron_output(inputs, weights, bias)\n",
    "    expected = expected_outputs[i]\n",
    "    status = \"✓\" if output == expected else \"✗\"\n",
    "    print(f\"   {inputs[0]}    |    {inputs[1]}    |    {z:6.1f}    |   {output}    |    {expected}     {status}\")\n",
    "\n",
    "print(\"\\nThe perceptron successfully implements an AND gate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the perceptron makes decisions\n",
    "def plot_perceptron_decision(weights, bias, title=\"Perceptron Decision Boundary\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a grid of points\n",
    "    x1 = np.linspace(-2, 2, 100)\n",
    "    x2 = np.linspace(-2, 2, 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    \n",
    "    # Calculate decision boundary: w1*x1 + w2*x2 + b = 0\n",
    "    # Solving for x2: x2 = -(w1*x1 + b) / w2\n",
    "    if weights[1] != 0:\n",
    "        boundary_x2 = -(weights[0] * x1 + bias) / weights[1]\n",
    "        plt.plot(x1, boundary_x2, 'k-', linewidth=3, label='Decision Boundary')\n",
    "    \n",
    "    # Calculate outputs for all grid points\n",
    "    Z = np.zeros_like(X1)\n",
    "    for i in range(X1.shape[0]):\n",
    "        for j in range(X1.shape[1]):\n",
    "            z_val = weights[0] * X1[i,j] + weights[1] * X2[i,j] + bias\n",
    "            Z[i,j] = step_function(z_val)\n",
    "    \n",
    "    # Plot decision regions\n",
    "    plt.contourf(X1, X2, Z, levels=[0, 0.5, 1], colors=['lightcoral', 'lightblue'], alpha=0.6)\n",
    "    \n",
    "    # Plot the test points\n",
    "    test_points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    colors = ['red', 'red', 'red', 'blue']  # AND gate: only [1,1] is blue (output=1)\n",
    "    \n",
    "    for i, (point, color) in enumerate(zip(test_points, colors)):\n",
    "        plt.scatter(point[0], point[1], c=color, s=200, edgecolors='black', linewidth=2)\n",
    "        plt.annotate(f'({point[0]},{point[1]})', (point[0], point[1]), \n",
    "                    xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "    \n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.xlabel('Input 1 (x₁)', fontsize=12)\n",
    "    plt.ylabel('Input 2 (x₂)', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add region labels\n",
    "    plt.text(0.2, 0.2, 'Output = 0\\n(Red Region)', fontsize=11, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.8))\n",
    "    plt.text(1.2, 1.2, 'Output = 1\\n(Blue Region)', fontsize=11,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize the AND gate perceptron\n",
    "plot_perceptron_decision(weights, bias, \"Perceptron Decision Boundary: AND Gate\")\n",
    "\n",
    "print(f\"Decision boundary equation: {weights[0]:.1f}x₁ + {weights[1]:.1f}x₂ + {bias:.1f} = 0\")\n",
    "print(\"Points above/right of the line output 1, points below/left output 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning Algorithm\n",
    "\n",
    "The beauty of the perceptron is that it can **learn** the correct weights automatically!\n",
    "\n",
    "### Learning Rule:\n",
    "For each training example:\n",
    "1. Make a prediction: $\\hat{y} = \\text{step}(\\mathbf{w}^T\\mathbf{x} + b)$\n",
    "2. Calculate error: $e = y - \\hat{y}$ (where $y$ is the true label)\n",
    "3. Update weights: $\\mathbf{w} = \\mathbf{w} + \\eta \\cdot e \\cdot \\mathbf{x}$\n",
    "4. Update bias: $b = b + \\eta \\cdot e$\n",
    "\n",
    "Where $\\eta$ (eta) is the **learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the perceptron learning algorithm\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, max_epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the perceptron\"\"\"\n",
    "        # Initialize weights and bias\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                # Forward pass\n",
    "                z = np.dot(X[i], self.weights) + self.bias\n",
    "                prediction = 1 if z >= 0 else 0\n",
    "                \n",
    "                # Calculate error\n",
    "                error = y[i] - prediction\n",
    "                \n",
    "                # Update weights and bias\n",
    "                if error != 0:  # Only update if there's an error\n",
    "                    self.weights += self.learning_rate * error * X[i]\n",
    "                    self.bias += self.learning_rate * error\n",
    "                \n",
    "                total_error += abs(error)\n",
    "            \n",
    "            self.errors.append(total_error)\n",
    "            \n",
    "            # Stop if perfect classification\n",
    "            if total_error == 0:\n",
    "                print(f\"Perfect classification achieved in {epoch + 1} epochs!\")\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return (z >= 0).astype(int)\n",
    "\n",
    "# Train a perceptron to learn the AND gate\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "\n",
    "# Create and train perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, max_epochs=100)\n",
    "print(\"Training perceptron to learn AND gate...\")\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Test the trained perceptron\n",
    "predictions = perceptron.predict(X_train)\n",
    "\n",
    "print(\"\\n=== Results ===\")\n",
    "print(\"Input | Expected | Predicted | Correct?\")\n",
    "print(\"-\" * 38)\n",
    "for i in range(len(X_train)):\n",
    "    correct = \"✓\" if predictions[i] == y_train[i] else \"✗\"\n",
    "    print(f\"{X_train[i]} |    {y_train[i]}     |     {predictions[i]}     |   {correct}\")\n",
    "\n",
    "print(f\"\\nLearned weights: {perceptron.weights}\")\n",
    "print(f\"Learned bias: {perceptron.bias:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning process\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Learning curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(perceptron.errors, 'b-', linewidth=2, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Error')\n",
    "plt.title('Perceptron Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_perceptron_decision(perceptron.weights, perceptron.bias, \"Learned Decision Boundary\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The perceptron successfully learned to separate the classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR Problem: Perceptron's Limitation\n",
    "\n",
    "In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons\" and showed a critical limitation: the perceptron cannot solve the XOR problem.\n",
    "\n",
    "### XOR (Exclusive OR) Truth Table:\n",
    "- (0, 0) → 0\n",
    "- (0, 1) → 1  \n",
    "- (1, 0) → 1\n",
    "- (1, 1) → 0\n",
    "\n",
    "The problem is that XOR is **not linearly separable** - you cannot draw a single straight line to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the XOR problem\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# XOR data\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR outputs\n",
    "\n",
    "# Plot 1: XOR problem visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue', 'blue', 'red']\n",
    "for i, (point, color, label) in enumerate(zip(X_xor, colors, y_xor)):\n",
    "    plt.scatter(point[0], point[1], c=color, s=200, edgecolors='black', linewidth=2)\n",
    "    plt.annotate(f'({point[0]},{point[1]})→{label}', (point[0], point[1]), \n",
    "                xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('XOR Problem: Not Linearly Separable')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Try to show that no single line can separate the classes\n",
    "plt.text(0.5, 1.3, 'No single straight line\\ncan separate red from blue!', \n",
    "         ha='center', fontsize=12, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "# Plot 2: Try training perceptron on XOR (it will fail)\n",
    "plt.subplot(1, 2, 2)\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, max_epochs=50)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "plt.plot(perceptron_xor.errors, 'r-', linewidth=2, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Error')\n",
    "plt.title('Perceptron Cannot Learn XOR')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The perceptron fails to learn XOR because it's not linearly separable.\")\n",
    "print(\"This limitation led to the 'AI Winter' of the 1970s-1980s.\")\n",
    "print(\"\\nSolution: Multi-layer networks (coming in future lessons!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of the Perceptron\n",
    "\n",
    "Despite its limitations, the perceptron is still useful for:\n",
    "\n",
    "### 1. Linearly Separable Problems\n",
    "- Simple binary classification\n",
    "- Email spam detection (linear features)\n",
    "- Sentiment analysis (simple cases)\n",
    "\n",
    "### 2. Building Blocks\n",
    "- Foundation for multi-layer networks\n",
    "- Understanding neural network basics\n",
    "- Educational purposes\n",
    "\n",
    "### 3. Real-World Examples\n",
    "- Early optical character recognition\n",
    "- Simple pattern recognition\n",
    "- Feature selection in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using perceptron for a real classification problem\n",
    "# Let's create a simple dataset where perceptron will work well\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate linearly separable data\n",
    "n_samples = 100\n",
    "X1 = np.random.randn(n_samples//2, 2) + np.array([2, 2])\n",
    "X2 = np.random.randn(n_samples//2, 2) + np.array([-2, -2])\n",
    "\n",
    "X = np.vstack([X1, X2])\n",
    "y = np.hstack([np.ones(n_samples//2), np.zeros(n_samples//2)])\n",
    "\n",
    "# Train perceptron\n",
    "perceptron_real = Perceptron(learning_rate=0.01, max_epochs=100)\n",
    "perceptron_real.fit(X, y)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot data points\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y == i\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, s=50, \n",
    "                label=f'Class {i}')\n",
    "\n",
    "# Plot decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = perceptron_real.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, colors=['lightcoral', 'lightblue'])\n",
    "plt.contour(xx, yy, Z, colors='black', linewidths=2, linestyles='--')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Perceptron on Linearly Separable Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = perceptron_real.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(\"The perceptron works perfectly on linearly separable data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Strengths of the Perceptron:\n",
    "1. **Simple and intuitive** - easy to understand and implement\n",
    "2. **Guaranteed convergence** - will find solution if one exists\n",
    "3. **Fast training** - linear in the number of examples\n",
    "4. **Interpretable** - weights show feature importance\n",
    "5. **Foundation** - basis for more complex neural networks\n",
    "\n",
    "### Limitations:\n",
    "1. **Linear separation only** - cannot solve XOR and similar problems\n",
    "2. **Binary output** - only 0 or 1\n",
    "3. **Step function** - not differentiable (limits learning methods)\n",
    "4. **No probabilistic output** - just hard classifications\n",
    "\n",
    "### Historical Impact:\n",
    "- **1958-1969**: Great excitement about perceptrons\n",
    "- **1969**: Minsky & Papert showed limitations → AI Winter\n",
    "- **1980s**: Multi-layer networks solved XOR problem\n",
    "- **Today**: Perceptrons still used as building blocks\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. Why was the XOR problem so devastating to AI research in the 1970s?\n",
    "2. Can you think of real-world problems that are linearly separable?\n",
    "3. How might we modify the perceptron to handle non-linear problems?\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: We'll explore **Activation Functions** - the key to making neural networks more powerful and flexible!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}